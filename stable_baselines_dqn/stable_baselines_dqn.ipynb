{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Sketchbook for Minesweeper solver with Stable Baselines3\n",
    "\n",
    "References\n",
    "> 1. [(medium) article for stable baselines](https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82)\n",
    "> 1. [(colab) example of medium article](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb)\n",
    "> 1. [(github) minesweeper gym environment](https://github.com/aylint/gym-minesweeper)\n",
    "\n",
    "Helps\n",
    "> 1. [(github) stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "> 1. [(github) stable-baselines3-contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n",
    "> 1. [(github) stable-baselines](https://github.com/hill-a/stable-baselines)\n",
    "> 1. [(doc) stable-baselines](https://stable-baselines.readthedocs.io/en/master/)\n",
    "> 1. [(doc) stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html)\n",
    "> 1. [(doc) stable-baselines3-contrib](https://sb3-contrib.readthedocs.io/en/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32163,
     "status": "ok",
     "timestamp": 1653442503910,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "bC6ssh-iDeWj",
    "outputId": "7dad0010-0612-4180-e6b1-2bd8e440b104"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWN44VDPN25I"
   },
   "source": [
    "## DQN of Stable Baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1653443597762,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "YZQ9HP09CLCu"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat, configure\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "#from stable_baselines3.her.her_replay_buffer import HerReplayBuffer\n",
    "from typing import Callable\n",
    "\n",
    "import minesweeper_gym_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1889491/1573905566.py:35: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_1889491/1573905566.py:37: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from six import StringIO\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# cell values, non-negatives indicate number of neighboring mines\n",
    "MINE = -1\n",
    "CLOSED = -2\n",
    "\n",
    "\n",
    "class MinesweeperModifiedEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"ansi\", \"human\"]}\n",
    "\n",
    "    def __init__(self, board_size=9, num_mines=10):\n",
    "        \"\"\"\n",
    "        Create a minesweeper game.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        board_size: int     shape of the board\n",
    "            - int: the same as (int, int)\n",
    "        num_mines: int   num mines on board\n",
    "        \"\"\"\n",
    "\n",
    "        self.board_size = board_size\n",
    "        self.num_mines = num_mines\n",
    "        self.board = self.place_mines(board_size, num_mines)\n",
    "        self.my_board = np.ones((board_size, board_size), dtype=int) * CLOSED\n",
    "        self.num_actions = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-2, high=9,\n",
    "                                            shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
    "        self.action_space = spaces.Discrete(self.board_size*self.board_size)\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n",
    "\n",
    "    def board2str(self, board, end='\\n'):\n",
    "        \"\"\"\n",
    "        Format a board as a string\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        board : np.array\n",
    "        end : str\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        s : str\n",
    "        \"\"\"\n",
    "        s = ''\n",
    "        for x in range(board.shape[1]):\n",
    "            for y in range(board.shape[2]):\n",
    "                s += str(board[0][x][y]) + '\\t'\n",
    "            s += end\n",
    "        #s += end\n",
    "        return s[:-len(end)]\n",
    "\n",
    "\n",
    "    def is_new_move(self, my_board, x, y):\n",
    "        \"\"\" return true if this is not an already clicked place\"\"\"\n",
    "        return my_board[0, x, y] == CLOSED\n",
    "\n",
    "\n",
    "    def is_valid(self, x, y):\n",
    "        \"\"\" returns if the coordinate is valid\"\"\"\n",
    "        return (x >= 0) & (x < self.board_size) & (y >= 0) & (y < self.board_size)\n",
    "\n",
    "\n",
    "    def is_win(self, my_board):\n",
    "        \"\"\" return if the game is won \"\"\"\n",
    "        return np.count_nonzero(my_board == CLOSED) == self.num_mines\n",
    "\n",
    "\n",
    "    def is_mine(self, board, x, y):\n",
    "        \"\"\"return if the coordinate has a mine or not\"\"\"\n",
    "        return board[0, x, y] == MINE\n",
    "\n",
    "\n",
    "    def place_mines(self, board_size, num_mines):\n",
    "        \"\"\"generate a board, place mines randomly\"\"\"\n",
    "        mines_placed = 0\n",
    "        board = np.zeros((1, board_size, board_size), dtype=int)\n",
    "        while mines_placed < num_mines:\n",
    "            rnd = randint(0, board_size * board_size)\n",
    "            x = int(rnd / board_size)\n",
    "            y = int(rnd % board_size)\n",
    "            if self.is_valid(x, y):\n",
    "                if not self.is_mine(board, x, y):\n",
    "                    board[0, x, y] = MINE\n",
    "                    mines_placed += 1\n",
    "        return board\n",
    "\n",
    "    def count_neighbour_mines(self, x, y):\n",
    "        \"\"\"return number of mines in neighbour cells given an x-y coordinate\n",
    "\n",
    "            Cell -->Current Cell(row, col)\n",
    "            N -->  North(row - 1, col)\n",
    "            S -->  South(row + 1, col)\n",
    "            E -->  East(row, col + 1)\n",
    "            W -->  West(row, col - 1)\n",
    "            N.E --> North - East(row - 1, col + 1)\n",
    "            N.W --> North - West(row - 1, col - 1)\n",
    "            S.E --> South - East(row + 1, col + 1)\n",
    "            S.W --> South - West(row + 1, col - 1)\n",
    "        \"\"\"\n",
    "        neighbour_mines = 0\n",
    "        for _x in range(x - 1, x + 2):\n",
    "            for _y in range(y - 1, y + 2):\n",
    "                if self.is_valid(_x, _y):\n",
    "                    if self.is_mine(self.board, _x, _y):\n",
    "                        neighbour_mines += 1\n",
    "        return neighbour_mines\n",
    "    \n",
    "    def possibility_neighbour_mine(self, x, y):\n",
    "        \"\"\"return possibility of mines in neighbour cells given an x-y coordinate\n",
    "        \"\"\"\n",
    "        neighbour_mines = False\n",
    "        rand = np.random.random() # random value b/w 0 & 1\n",
    "        for _x in range(x - 1, x + 2):\n",
    "            for _y in range(y - 1, y + 2):\n",
    "                if self.is_valid(_x, _y):\n",
    "                    if self.is_mine(self.board, _x, _y):\n",
    "                        neighbour_mines = True\n",
    "                        break\n",
    "        if neighbour_mines:\n",
    "            return 1\n",
    "        return 1 if rand < 0.1 else 0\n",
    "\n",
    "    def get_next_state(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next state.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)   visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        game_over : (bool) true if game over\n",
    "\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        mine_point = False\n",
    "        #game_over = False\n",
    "        if self.is_mine(self.board, x, y):\n",
    "            my_board[0, x, y] = MINE\n",
    "            mine_point = True\n",
    "            #game_over = True\n",
    "        else:\n",
    "            #my_board[0, x, y] = self.count_neighbour_mines(x, y)\n",
    "            my_board[0, x, y] = self.possibility_neighbour_mine(x, y)\n",
    "\n",
    "        self.my_board = my_board\n",
    "        #return my_board, game_over\n",
    "        return my_board, mine_point\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset a new game episode. See gym.Env.reset()\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array, int)    next board\n",
    "        \"\"\"\n",
    "        self.my_board = np.ones((1, self.board_size, self.board_size), dtype=int) * CLOSED\n",
    "        self.board = self.place_mines(self.board_size, self.num_mines)\n",
    "        self.num_actions = 0\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)\n",
    "\n",
    "        return self.my_board\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        See gym.Env.step().\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        action : np.array    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next board\n",
    "        reward : float        the reward for action\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}             {'valid_actions': valid_actions} - a binary vector,\n",
    "                                where false cells' values are already known to observer\n",
    "        \"\"\"\n",
    "        state = self.my_board\n",
    "        x = int(action / self.board_size)\n",
    "        y = int(action % self.board_size)\n",
    "\n",
    "        # test valid action - uncomment this part to test your action filter if needed\n",
    "        # if bool(self.valid_actions[action]) is False:\n",
    "        #    raise Exception(\"Invalid action was selected! Action Filter: {}, \"\n",
    "        #                    \"action taken: {}\".format(self.valid_actions, action))\n",
    "\n",
    "        next_state, reward, done, info = self.next_step(state, x, y)\n",
    "        self.my_board = next_state\n",
    "        self.num_actions += 1\n",
    "        self.valid_actions = (next_state.flatten() == CLOSED)\n",
    "        info['valid_actions'] = self.valid_actions\n",
    "        info['num_actions'] = self.num_actions\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def is_guess(self, my_board, x, y):\n",
    "        for _x in range(x-1, x+2):\n",
    "            for _y in range(y-1, y+2):\n",
    "                if self.is_valid(_x, _y):\n",
    "                    if not self.is_new_move(my_board, _x, _y):\n",
    "                        if (x != _x) or (y != _y):\n",
    "                            return False\n",
    "        return True\n",
    "                    \n",
    "\n",
    "    def next_step(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next observation, reward, done, and info.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)    visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        reward : float               the reward\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        #win_or_lose = False\n",
    "        reward = 0\n",
    "        done = False\n",
    "        t_b = False\n",
    "        info = {'is_success': False}\n",
    "        #if self.num_actions > my_board.shape[0] * my_board.shape[1]:\n",
    "        #    reward = -0.1\n",
    "            \n",
    "        if not self.is_new_move(my_board, x, y):\n",
    "            reward = -0.3\n",
    "            return my_board, reward, False, info\n",
    "        is_guess_b = self.is_guess(my_board, x, y) # if guess\n",
    "\n",
    "        state, mine_point = self.get_next_state(my_board, x, y)\n",
    "\n",
    "        my_board_flatten = my_board.flatten()\n",
    "        board_flatten = self.board.flatten()\n",
    "        if np.array_equal(np.where(my_board_flatten == CLOSED),\n",
    "                          np.where(board_flatten == MINE)): # Win\n",
    "            reward = 1\n",
    "            done = True\n",
    "            info['is_success'] = True\n",
    "        elif (np.sum(my_board == CLOSED) == 0) or \\\n",
    "                (np.sum(board_flatten[np.where(my_board_flatten == CLOSED)] != MINE) == 0): # Lose\n",
    "            #reward = 0\n",
    "            done = True\n",
    "        elif mine_point: # Step on a mine\n",
    "            reward = -0.5\n",
    "        elif is_guess_b: # Guess\n",
    "            reward = -0.3\n",
    "        else: # Progress\n",
    "            reward = 0.5\n",
    "            \n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        See gym.Env.render().\n",
    "        \"\"\"\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "        s = self.board2str(self.my_board)\n",
    "        outfile.write(s)\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "\n",
    "env = MinesweeperModifiedEnv(4, 3)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1653444568193,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "ONFKJjMF49T5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3131429/3781723117.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_3131429/3781723117.py:97: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from six import StringIO\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# default : easy board\n",
    "BOARD_SIZE = 5\n",
    "NUM_MINES = 3\n",
    "\n",
    "# cell values, non-negatives indicate number of neighboring mines\n",
    "MINE = -1\n",
    "CLOSED = -2\n",
    "\n",
    "\n",
    "def board2str(board, end='\\n'):\n",
    "    \"\"\"\n",
    "    Format a board as a string\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    board : np.array\n",
    "    end : str\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    s : str\n",
    "    \"\"\"\n",
    "    s = ''\n",
    "    for x in range(board.shape[1]):\n",
    "        for y in range(board.shape[2]):\n",
    "            s += str(board[0][x][y]) + '\\t'\n",
    "        s += end\n",
    "    #s += end\n",
    "    return s[:-len(end)]\n",
    "\n",
    "\n",
    "def is_new_move(my_board, x, y):\n",
    "    \"\"\" return true if this is not an already clicked place\"\"\"\n",
    "    return my_board[0, x, y] == CLOSED\n",
    "\n",
    "\n",
    "def is_valid(x, y):\n",
    "    \"\"\" returns if the coordinate is valid\"\"\"\n",
    "    return (x >= 0) & (x < BOARD_SIZE) & (y >= 0) & (y < BOARD_SIZE)\n",
    "\n",
    "\n",
    "def is_win(my_board):\n",
    "    \"\"\" return if the game is won \"\"\"\n",
    "    return np.count_nonzero(my_board == CLOSED) == NUM_MINES\n",
    "\n",
    "\n",
    "def is_mine(board, x, y):\n",
    "    \"\"\"return if the coordinate has a mine or not\"\"\"\n",
    "    return board[0, x, y] == MINE\n",
    "\n",
    "\n",
    "def place_mines(board_size, num_mines):\n",
    "    \"\"\"generate a board, place mines randomly\"\"\"\n",
    "    mines_placed = 0\n",
    "    board = np.zeros((1, board_size, board_size), dtype=int)\n",
    "    while mines_placed < num_mines:\n",
    "        rnd = randint(0, board_size * board_size)\n",
    "        x = int(rnd / board_size)\n",
    "        y = int(rnd % board_size)\n",
    "        if is_valid(x, y):\n",
    "            if not is_mine(board, x, y):\n",
    "                board[0, x, y] = MINE\n",
    "                mines_placed += 1\n",
    "    return board\n",
    "\n",
    "class MinesweeperDiscreetEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"ansi\", \"human\"]}\n",
    "\n",
    "    def __init__(self, board_size=BOARD_SIZE, num_mines=NUM_MINES):\n",
    "        \"\"\"\n",
    "        Create a minesweeper game.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        board_size: int     shape of the board\n",
    "            - int: the same as (int, int)\n",
    "        num_mines: int   num mines on board\n",
    "        \"\"\"\n",
    "\n",
    "        self.board_size = board_size\n",
    "        self.num_mines = num_mines\n",
    "        self.board = place_mines(board_size, num_mines)\n",
    "        self.my_board = np.ones((board_size, board_size), dtype=int) * CLOSED\n",
    "        self.num_actions = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-2, high=9,\n",
    "                                            shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
    "        self.action_space = spaces.Discrete(self.board_size*self.board_size)\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n",
    "\n",
    "    def count_neighbour_mines(self, x, y):\n",
    "        \"\"\"return number of mines in neighbour cells given an x-y coordinate\n",
    "\n",
    "            Cell -->Current Cell(row, col)\n",
    "            N -->  North(row - 1, col)\n",
    "            S -->  South(row + 1, col)\n",
    "            E -->  East(row, col + 1)\n",
    "            W -->  West(row, col - 1)\n",
    "            N.E --> North - East(row - 1, col + 1)\n",
    "            N.W --> North - West(row - 1, col - 1)\n",
    "            S.E --> South - East(row + 1, col + 1)\n",
    "            S.W --> South - West(row + 1, col - 1)\n",
    "        \"\"\"\n",
    "        neighbour_mines = 0\n",
    "        for _x in range(x - 1, x + 2):\n",
    "            for _y in range(y - 1, y + 2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if is_mine(self.board, _x, _y):\n",
    "                        neighbour_mines += 1\n",
    "        return neighbour_mines\n",
    "\n",
    "    def open_neighbour_cells(self, my_board, x, y):\n",
    "        \"\"\"return number of mines in neighbour cells given an x-y coordinate\n",
    "\n",
    "            Cell -->Current Cell(row, col)\n",
    "            N -->  North(row - 1, col)\n",
    "            S -->  South(row + 1, col)\n",
    "            E -->  East(row, col + 1)\n",
    "            W -->  West(row, col - 1)\n",
    "            N.E --> North - East(row - 1, col + 1)\n",
    "            N.W --> North - West(row - 1, col - 1)\n",
    "            S.E --> South - East(row + 1, col + 1)\n",
    "            S.W --> South - West(row + 1, col - 1)\n",
    "        \"\"\"\n",
    "        for _x in range(x-1, x+2):\n",
    "            for _y in range(y-1, y+2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if is_new_move(my_board, _x, _y):\n",
    "                        my_board[0, _x, _y] = self.count_neighbour_mines(_x, _y)\n",
    "                        if my_board[0, _x, _y] == 0:\n",
    "                            my_board = self.open_neighbour_cells(my_board, _x, _y)\n",
    "        return my_board\n",
    "\n",
    "    def get_next_state(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next state.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)   visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        game_over : (bool) true if game over\n",
    "\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        game_over = False\n",
    "        if is_mine(self.board, x, y):\n",
    "            my_board[0, x, y] = MINE\n",
    "            game_over = True\n",
    "        else:\n",
    "            my_board[0, x, y] = self.count_neighbour_mines(x, y)\n",
    "            if my_board[0, x, y] == 0:\n",
    "                my_board = self.open_neighbour_cells(my_board, x, y)\n",
    "        self.my_board = my_board\n",
    "        return my_board, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset a new game episode. See gym.Env.reset()\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array, int)    next board\n",
    "        \"\"\"\n",
    "        self.my_board = np.ones((1, self.board_size, self.board_size), dtype=int) * CLOSED\n",
    "        self.board = place_mines(self.board_size, self.num_mines)\n",
    "        self.num_actions = 0\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)\n",
    "\n",
    "        return self.my_board\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        See gym.Env.step().\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        action : np.array    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next board\n",
    "        reward : float        the reward for action\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}             {'valid_actions': valid_actions} - a binary vector,\n",
    "                                where false cells' values are already known to observer\n",
    "        \"\"\"\n",
    "        state = self.my_board\n",
    "        x = int(action / self.board_size)\n",
    "        y = int(action % self.board_size)\n",
    "\n",
    "        # test valid action - uncomment this part to test your action filter if needed\n",
    "        # if bool(self.valid_actions[action]) is False:\n",
    "        #    raise Exception(\"Invalid action was selected! Action Filter: {}, \"\n",
    "        #                    \"action taken: {}\".format(self.valid_actions, action))\n",
    "\n",
    "        next_state, reward, done, info = self.next_step(state, x, y)\n",
    "        self.my_board = next_state\n",
    "        self.num_actions += 1\n",
    "        self.valid_actions = (next_state.flatten() == CLOSED)\n",
    "        info['valid_actions'] = self.valid_actions\n",
    "        info['num_actions'] = self.num_actions\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def is_guess(self, my_board, x, y):\n",
    "        for _x in range(x-1, x+2):\n",
    "            for _y in range(y-1, y+2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if not is_new_move(my_board, _x, _y):\n",
    "                        if (x != _x) or (y != _y):\n",
    "                            return False\n",
    "        return True\n",
    "                    \n",
    "\n",
    "    def next_step(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next observation, reward, done, and info.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)    visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        reward : float               the reward\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        #win_or_lose = False\n",
    "        reward = 0\n",
    "        done = False\n",
    "        t_b = False\n",
    "        info = {'is_success': False}\n",
    "        #if self.num_actions > my_board.shape[0] * my_board.shape[1]:\n",
    "        #    reward = -0.1\n",
    "            \n",
    "        if not is_new_move(my_board, x, y):\n",
    "            reward += -0.3\n",
    "            return my_board, reward, False, info\n",
    "        elif self.is_guess(my_board, x, y): # if guess\n",
    "            t_b = True\n",
    "\n",
    "        state, game_over = self.get_next_state(my_board, x, y)\n",
    "\n",
    "        if game_over:\n",
    "            reward += -1\n",
    "            done = True\n",
    "            #return state, -100, True, {}\n",
    "        elif is_win(state):\n",
    "            reward += 1\n",
    "            done = True\n",
    "            info['is_success'] = True\n",
    "            #return state, 1000, True, {}\n",
    "        elif t_b: # if guess\n",
    "            reward += -0.3\n",
    "        else: # progress\n",
    "            reward += 0.9\n",
    "            #return state, 0, False, {}\n",
    "            \n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        See gym.Env.render().\n",
    "        \"\"\"\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "        s = board2str(self.my_board)\n",
    "        outfile.write(s)\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "\n",
    "env = MinesweeperDiscreetEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653436025115,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "MthXfoFIkDL-"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1653436083595,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "hnv4G8QQkAcX",
    "outputId": "02545a85-eba9-4235-c565-277622e7317b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1\n",
      "0\t1\t1\t1\t\n",
      "0\t1\t-1\t1\t\n",
      "1\t1\t1\t1\t\n",
      "-2\t-2\t1\t0\tFalse\n",
      "[[[False False False False]\n",
      "  [False False  True False]\n",
      "  [False False False False]\n",
      "  [ True  True False False]]]\n",
      "\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "if done:\n",
    "    obs = env.reset()\n",
    "    print('New Game!')\n",
    "row, col = 3, 2\n",
    "\n",
    "action = 4 * row + col\n",
    "obs, reward, done, info = env.step(action)\n",
    "print('reward: {}'.format(reward))\n",
    "env.render()\n",
    "print(info.get('is_success'))\n",
    "print(env.board == -1)\n",
    "if done:\n",
    "    print('\\nGame Over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1653444581265,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "QzSFVG-1cnFu"
   },
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.sample()[None].shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_dim, features_dim, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1653443603558,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "zKDTOVah6h64"
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    #lr0 = initial_value\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining: = 1.0 - (num_timesteps / total_timesteps)\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        if progress_remaining > 0.8:\n",
    "            return initial_value\n",
    "        else:\n",
    "            return progress_remaining * initial_value * 1.25\n",
    "        #return progress_remaining * initial_value\n",
    "        #nonlocal lr0\n",
    "        #lr0 = max(0.001, lr0 * 0.99975) # 0.99975\n",
    "        #return lr0\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1653444849304,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "zAUvfIZBCDdL"
   },
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy', env, \n",
    "            learning_rate=linear_schedule(0.001), \n",
    "            #policy_kwargs=dict(activation_fn=nn.ReLU,\n",
    "            #                   net_arch=[256, 256, 256, 256, 512, 512]), \n",
    "            policy_kwargs=policy_kwargs,\n",
    "            batch_size=64, \n",
    "            gamma=0.1, \n",
    "            train_freq=(1, 'episode'), \n",
    "            learning_starts=1,\n",
    "            #buffer_size=4,\n",
    "            exploration_fraction=0.16, \n",
    "            exploration_initial_eps=0.95, \n",
    "            exploration_final_eps=0.01,\n",
    "            tensorboard_log=\"./dqn_tensorboard/\", verbose=0\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ASA9d8QrDtQ0"
   },
   "outputs": [],
   "source": [
    "#callback = TensorboardCallback(eval_env=MinesweeperDiscreetEnv())\n",
    "model.learn(total_timesteps=int(1e5), \n",
    "            log_interval=10,\n",
    "            tb_log_name='test_env',\n",
    "            #eval_log_path='eval_test',\n",
    "            reset_num_timesteps=True)\n",
    "model.save(\"dqn_minesweeper_test_env\")\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.88 Num episodes: 1000\n",
      "mean_reward: 1.34\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"dqn_minesweeper_s4m1\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3131429/1072893841.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_3131429/1072893841.py:97: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.57 Num episodes: 1000\n",
      "mean_reward: -4.82\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"dqn_minesweeper_s4m2\")\n",
    "episode_rewards, episode_wins = evaluate(model,\n",
    "                                         env=MinesweeperDiscreetEnv(),\n",
    "                                         num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3131429/1493132668.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_3131429/1493132668.py:97: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.54 Num episodes: 1000\n",
      "mean_reward: -0.04\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"dqn_minesweeper_s5m3\")\n",
    "episode_rewards, episode_wins = evaluate(model,\n",
    "                                         env=MinesweeperDiscreetEnv(),\n",
    "                                         num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3131429/1493132668.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_3131429/1493132668.py:97: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.27 Num episodes: 1000\n",
      "mean_reward: -10.81\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"dqn_minesweeper_s5m3_wr0.29\")\n",
    "episode_rewards, episode_wins = evaluate(model,\n",
    "                                         env=MinesweeperDiscreetEnv(),\n",
    "                                         num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env,\n",
    "            #learning_rate=linear_schedule(0.001),\n",
    "            #n_step,\n",
    "            batch_size=64,\n",
    "            #n_epochs = ,\n",
    "            gamma=0.1,\n",
    "            tensorboard_log=\"./ppo_tensorboard/\",\n",
    "            #create_eval_env=True,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=0,\n",
    "            #seed=23,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [228]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2e5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms5m3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#eval_env=MinesweeperDiscreetEnv(),\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#eval_freq=100,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#n_eval_episodes=10,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#eval_log_path='s5m4'\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_minesweeper_s5m3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:304\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    293\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    246\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 250\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:170\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs_tensor)\n\u001b[0;32m--> 170\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    173\u001b[0m clipped_actions \u001b[38;5;241m=\u001b[39m actions\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(2e5), \n",
    "            log_interval=10,\n",
    "            tb_log_name='s5m3', \n",
    "            #eval_env=MinesweeperDiscreetEnv(),\n",
    "            #eval_freq=100,\n",
    "            #n_eval_episodes=10,\n",
    "            #eval_log_path='s5m4'\n",
    "            reset_num_timesteps=True)\n",
    "model.save(\"ppo_minesweeper_s5m3\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_minesweeper_s5m4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m episode_rewards, episode_wins \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, env, num_episodes)\u001b[0m\n\u001b[1;32m     12\u001b[0m episode_rewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m     episode_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:562\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m     observation: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/policies.py:335\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# if state is None:\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 335\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    338\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/policies.py:254\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 254\u001b[0m observation \u001b[38;5;241m=\u001b[39m obs_as_tensor(observation, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, vectorized_env\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/policies.py:152\u001b[0m, in \u001b[0;36mBaseModel.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdevice\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124;03m\"\"\"Infer which device this policy lives on by inspecting its parameters.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    If it has no parameters, the 'cpu' device is used as a fallback.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    :return:\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1535\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \n\u001b[1;32m   1517\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \n\u001b[1;32m   1534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;124;03mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1560\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse)\n\u001b[0;32m-> 1561\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m elem\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m memo\u001b[38;5;241m.\u001b[39madd(v)\n\u001b[0;32m-> 1511\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m name, v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"ppo_minesweeper_s5m3\",\n",
    "                 env=MinesweeperDiscreetEnv(),\n",
    "                 device='cuda')\n",
    "episode_rewards, episode_wins = evaluate(model, env=model.get_env(), num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1889491/1573905566.py:35: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_1889491/1573905566.py:37: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m MinesweeperModifiedEnv(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, \n\u001b[1;32m      3\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39mlinear_schedule(\u001b[38;5;241m0.001\u001b[39m), \n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m#policy_kwargs=dict(activation_fn=nn.ReLU,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m             tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./custom_dqn_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m            )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5e5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms4m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#eval_log_path='eval_test',\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_dqn_minesweeper_s4m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:258\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    247\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDQN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    344\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 347\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:577\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    580\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:408\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     unscaled_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs)])\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     unscaled_action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox):\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:242\u001b[0m, in \u001b[0;36mDQN.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    240\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     action, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, state\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/common/policies.py:338\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    335\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 338\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Convert to numpy\u001b[39;00m\n\u001b[1;32m    340\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/dqn/policies.py:178\u001b[0m, in \u001b[0;36mDQNPolicy._predict\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/dqn/policies.py:69\u001b[0m, in \u001b[0;36mQNetwork._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: th\u001b[38;5;241m.\u001b[39mTensor, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 69\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/stable_baselines3/dqn/policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Predict the q-values.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    :param obs: Observation\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/sketchbook/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = MinesweeperModifiedEnv(4, 1)\n",
    "model = DQN('CnnPolicy', env, \n",
    "            learning_rate=linear_schedule(0.001), \n",
    "            #policy_kwargs=dict(activation_fn=nn.ReLU,\n",
    "            #                   net_arch=[256, 256, 256, 256, 512, 512]), \n",
    "            policy_kwargs=policy_kwargs,\n",
    "            batch_size=64, \n",
    "            gamma=0.1, \n",
    "            #train_freq=(1, 'episode'), \n",
    "            learning_starts=1,\n",
    "            #buffer_size=4,\n",
    "            exploration_fraction=0.16, \n",
    "            exploration_initial_eps=0.95, \n",
    "            exploration_final_eps=0.02,\n",
    "            tensorboard_log=\"./custom_dqn_tensorboard/\", verbose=0\n",
    "           )\n",
    "model.learn(total_timesteps=int(5e5), \n",
    "            log_interval=10,\n",
    "            tb_log_name='s4m1',\n",
    "            #eval_log_path='eval_test',\n",
    "            reset_num_timesteps=True)\n",
    "model.save(\"custom_dqn_minesweeper_s4m1\")\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1889491/1573905566.py:35: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_1889491/1573905566.py:37: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.0 Num episodes: 1000\n",
      "mean_reward: -59.64\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"custom_dqn_minesweeper_s4m1w8\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=MinesweeperModifiedEnv(4, 1), num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "2p29feu2O9WH"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, env, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward for the last 100 episodes\n",
    "    \"\"\"\n",
    "    episode_rewards = [0.0]\n",
    "    episode_wins = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "        #if i % 100 == 1:\n",
    "        #    print('Playing episode {}'.format(i))\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_rewards[-1] += reward\n",
    "            if done:\n",
    "                episode_wins.append(info.get('is_success'))\n",
    "                break\n",
    "            elif info.get('num_actions') > 200:\n",
    "                #print('Episode {}. Over action in obs, action: \\n{}, {}'.format(i, obs, action))\n",
    "                episode_wins.append(False)\n",
    "                break\n",
    "    \"\"\"\n",
    "    # Compute mean reward for the last 100 episodes\n",
    "    mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
    "    print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
    "    \"\"\"\n",
    "    win_rate = round(np.mean(episode_wins), 2)\n",
    "    print(\"Win rates:\", win_rate, \"Num episodes:\", len(episode_wins))\n",
    "    \n",
    "    return episode_rewards, episode_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sb3_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [229]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msb3_contrib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TRPO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sb3_contrib'"
     ]
    }
   ],
   "source": [
    "from sb3_contrib import TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1652343522531,
     "user": {
      "displayName": "이경헌",
      "userId": "12732568205041741847"
     },
     "user_tz": -540
    },
    "id": "wP7-tRuxWG07",
    "outputId": "e382649f-b819-442d-cc6b-e8d1f8bf73e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = minesweeper_gym.MinesweeperDiscreetEnv()\n",
    "\n",
    "model = TRPO('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12961,
     "status": "ok",
     "timestamp": 1652343537092,
     "user": {
      "displayName": "이경헌",
      "userId": "12732568205041741847"
     },
     "user_tz": -540
    },
    "id": "6BTG23KUWG08",
    "outputId": "e48cb2f4-59a7-4063-db45-4553c3807668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -99.0 Num episodes: 931\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46902,
     "status": "ok",
     "timestamp": 1652343585630,
     "user": {
      "displayName": "이경헌",
      "userId": "12732568205041741847"
     },
     "user_tz": -540
    },
    "id": "nwhHVhR2WG08",
    "outputId": "98c64c31-9be0-41e9-caac-70acc1d1a0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 10.7     |\n",
      "|    ep_rew_mean            | -100     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 445      |\n",
      "|    iterations             | 10       |\n",
      "|    time_elapsed           | 45       |\n",
      "|    total_timesteps        | 20480    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | -0.0141  |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.000774 |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 9        |\n",
      "|    policy_objective       | 0.0703   |\n",
      "|    value_loss             | 3.26e+03 |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e4), log_interval=10)\n",
    "# Save the agent\n",
    "model.save(\"trpo_minesweeper\")\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3NZEA6gWG08"
   },
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = TRPO.load(\"trpo_minesweeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12391,
     "status": "ok",
     "timestamp": 1652343729123,
     "user": {
      "displayName": "이경헌",
      "userId": "12732568205041741847"
     },
     "user_tz": -540
    },
    "id": "MMguiX5zWG08",
    "outputId": "d00ed61f-97ab-4c5a-c770-a5880d37d1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -99.0 Num episodes: 865\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward = evaluate(model, num_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzRz8Iv-XSAX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d2atE5vL8L2g",
    "BEazdwPFKZun",
    "9x47HdO6KPQ8",
    "Nsju026AQg0z",
    "AeDD26lyVwJP"
   ],
   "name": "sketchbook.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb",
     "timestamp": 1652336425341
    }
   ]
  },
  "kernelspec": {
   "display_name": "sketchbook",
   "language": "python",
   "name": "sketchbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
