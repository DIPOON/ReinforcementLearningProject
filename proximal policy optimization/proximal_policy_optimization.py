# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0i8Z4hmBkUj-kDJA2KmkLRyFYvWdK5a

function ClickConnect(){
    console.log("1분마다 코랩 연결 끊김 방지"); 
    document.querySelector("#top-toolbar > colab-connect-button").shadowRoot.querySelector("#connect-icon").click();
}
setInterval(ClickConnect, 1000 * 60);
"""

# Commented out IPython magic to ensure Python compatibility.
import sys, os

if 'google.colab' in sys.modules:
    from google.colab import drive
    drive.mount('/content/drive')
    sys.path.append('/content/drive/My Drive/RL_Project')
#     %cd /content/drive/My Drive/RL_Project

# %tensorflow_version 1.x

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
!pip install stable-baselines[mpi]==2.10.0

from six import StringIO
from random import randint

import numpy as np
import gym
from gym import spaces

win_list = []

# default : easy board
BOARD_SIZE = 9
NUM_MINES = 10

# cell values, non-negatives indicate number of neighboring mines
MINE = -1
CLOSED = -2


def board2str(board, end='\n'):
    """
    Format a board as a string
    Parameters
    ----
    board : np.array
    end : str
    Returns
    ----
    s : str
    """
    s = ''
    for x in range(board.shape[0]):
        for y in range(board.shape[1]):
            s += str(board[x][y]) + '\t'
        s += end
    return s[:-len(end)]


def is_new_move(my_board, x, y):
    """ return true if this is not an already clicked place"""
    return my_board[x, y] == CLOSED


def is_valid(x, y):
    """ returns if the coordinate is valid"""
    return (x >= 0) & (x < BOARD_SIZE) & (y >= 0) & (y < BOARD_SIZE)


def is_win(my_board):
    """ return if the game is won """
    return np.count_nonzero(my_board == CLOSED) == NUM_MINES


def is_mine(board, x, y):
    """return if the coordinate has a mine or not"""
    return board[x, y] == MINE


def place_mines(board_size, num_mines):
    """generate a board, place mines randomly"""
    mines_placed = 0
    board = np.zeros((board_size, board_size), dtype=int)
    while mines_placed < num_mines:
        rnd = randint(0, board_size * board_size)
        x = int(rnd / board_size)
        y = int(rnd % board_size)
        if is_valid(x, y):
            if not is_mine(board, x, y):
                board[x, y] = MINE
                mines_placed += 1
    return board

class MinesweeperDiscreetEnv(gym.Env):
    metadata = {"render.modes": ["ansi", "human"]}

    def __init__(self, board_size=BOARD_SIZE, num_mines=NUM_MINES):
        """
        Create a minesweeper game.
        Parameters
        ----
        board_size: int     shape of the board
            - int: the same as (int, int)
        num_mines: int   num mines on board
        """

        self.board_size = board_size
        self.num_mines = num_mines
        self.board = place_mines(board_size, num_mines)
        self.my_board = np.ones((board_size, board_size), dtype=int) * CLOSED
        self.num_actions = 0

        self.observation_space = spaces.Box(low=-2, high=9,
                                            shape=(self.board_size, self.board_size), dtype=np.int)
        self.action_space = spaces.Discrete(self.board_size*self.board_size)
        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)

    def count_neighbour_mines(self, x, y):
        """return number of mines in neighbour cells given an x-y coordinate
            Cell -->Current Cell(row, col)
            N -->  North(row - 1, col)
            S -->  South(row + 1, col)
            E -->  East(row, col + 1)
            W -->  West(row, col - 1)
            N.E --> North - East(row - 1, col + 1)
            N.W --> North - West(row - 1, col - 1)
            S.E --> South - East(row + 1, col + 1)
            S.W --> South - West(row + 1, col - 1)
        """
        neighbour_mines = 0
        for _x in range(x - 1, x + 2):
            for _y in range(y - 1, y + 2):
                if is_valid(_x, _y):
                    if is_mine(self.board, _x, _y):
                        neighbour_mines += 1
        return neighbour_mines

    def open_neighbour_cells(self, my_board, x, y):
        """return number of mines in neighbour cells given an x-y coordinate
            Cell -->Current Cell(row, col)
            N -->  North(row - 1, col)
            S -->  South(row + 1, col)
            E -->  East(row, col + 1)
            W -->  West(row, col - 1)
            N.E --> North - East(row - 1, col + 1)
            N.W --> North - West(row - 1, col - 1)
            S.E --> South - East(row + 1, col + 1)
            S.W --> South - West(row + 1, col - 1)
        """
        for _x in range(x-1, x+2):
            for _y in range(y-1, y+2):
                if is_valid(_x, _y):
                    if is_new_move(my_board, _x, _y):
                        my_board[_x, _y] = self.count_neighbour_mines(_x, _y)
                        if my_board[_x, _y] == 0:
                            my_board = self.open_neighbour_cells(my_board, _x, _y)
        return my_board

    def get_next_state(self, state, x, y):
        """
        Get the next state.
        Parameters
        ----
        state : (np.array)   visible board
        x : int    location
        y : int    location
        Returns
        ----
        next_state : (np.array)    next visible board
        game_over : (bool) true if game over
        """
        my_board = state
        game_over = False
        if is_mine(self.board, x, y):
            my_board[x, y] = MINE
            game_over = True
        else:
            my_board[x, y] = self.count_neighbour_mines(x, y)
            if my_board[x, y] == 0:
                my_board = self.open_neighbour_cells(my_board, x, y)
        self.my_board = my_board
        return my_board, game_over

    def reset(self):
        """
        Reset a new game episode. See gym.Env.reset()
        Returns
        ----
        next_state : (np.array, int)    next board
        """
        self.my_board = np.ones((self.board_size, self.board_size), dtype=int) * CLOSED
        self.board = place_mines(self.board_size, self.num_mines)
        self.num_actions = 0
        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)

        return self.my_board

    def step(self, action):
        """
        See gym.Env.step().
        Parameters
        ----
        action : np.array    location
        Returns
        ----
        next_state : (np.array)    next board
        reward : float        the reward for action
        done : bool           whether the game end or not
        info : {}             {'valid_actions': valid_actions} - a binary vector,
                                where false cells' values are already known to observer
        """
        state = self.my_board
        x = int(action / self.board_size)
        y = int(action % self.board_size)

        # test valid action - uncomment this part to test your action filter if needed
        # if bool(self.valid_actions[action]) is False:
        #    raise Exception("Invalid action was selected! Action Filter: {}, "
        #                    "action taken: {}".format(self.valid_actions, action))

        next_state, reward, done, info = self.next_step(state, x, y)
        self.my_board = next_state
        self.num_actions += 1
        self.valid_actions = (next_state.flatten() == CLOSED)
        info['valid_actions'] = self.valid_actions
        info['num_actions'] = self.num_actions
        return next_state, reward, done, info

    def next_step(self, state, x, y):
        """
        Get the next observation, reward, done, and info.
        Parameters
        ----
        state : (np.array)    visible board
        x : int    location
        y : int    location
        Returns
        ----
        next_state : (np.array)    next visible board
        reward : float               the reward
        done : bool           whether the game end or not
        info : {}
        """
        my_board = state
        if not is_new_move(my_board, x, y):
            return my_board, -1, False, {}
        while True:
            state, game_over = self.get_next_state(my_board, x, y)
            if not game_over:
                if is_win(state):
                    win_list.append(1)
                    return state, 1000, True, {}
                else:#Progress
                    return state, 1, False, {}
            else:#Lose
                win_list.append(0)
                return state, -100, True, {}

    def render(self, mode='human'):
        """
        See gym.Env.render().
        """
        outfile = StringIO() if mode == 'ansi' else sys.stdout
        s = board2str(self.my_board)
        outfile.write(s)
        if mode != 'human':
            return outfile

from stable_baselines.common.env_checker import check_env

env = MinesweeperDiscreetEnv()
check_env(env, warn=True)

import gym
import numpy as np

from stable_baselines3 import DQN, PPO, A2C
from sb3_contrib import TRPO
from stable_baselines3.common.env_checker import check_env

def evaluate(model, num_steps=1000):
  """
  Evaluate a RL agent
  :param model: (BaseRLModel object) the RL Agent
  :param num_steps: (int) number of timesteps to evaluate it
  :return: (float) Mean reward for the last 100 episodes
  """
  episode_rewards = [0.0]
  obs = env.reset()
  for i in range(num_steps):
      # _states are only useful when using LSTM policies
      action, _states = model.predict(obs)

      obs, reward, done, info = env.step(action)
      
      # Stats
      episode_rewards[-1] += reward
      if done:
          obs = env.reset()
          episode_rewards.append(0.0)
  # Compute mean reward for the last 100 episodes
  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)
  print("Mean reward:", mean_100ep_reward, "Num episodes:", len(episode_rewards))
  
  return mean_100ep_reward

observation = env.reset()
obs = env.reset()
env.render()

print(env.observation_space)

"""##PPO"""

env = MinesweeperDiscreetEnv()

model = PPO('MlpPolicy', env, verbose=1, tensorboard_log='/content/drive/My Drive/RL_Project')

# Random Agent, before training
mean_reward_before_train = evaluate(model, num_steps=10000)

print(len(win_list))
print(sum(win_list))

win_list = []

# Train the agent
model.learn(total_timesteps=int(1000000), log_interval=100)
# Save the agent
model.save("ppo_minesweeper")
del model  # delete trained model to demonstrate loading

#!tensorboard --logdir '/content/drive/My Drive/RL_Project'

print(len(win_list))
print(sum(win_list))

print(len(win_list[-2000:]))
print(sum(win_list[-2000:]))

# Load the trained agent
model = PPO.load("ppo_minesweeper")

# Evaluate the trained agent
mean_reward = evaluate(model, num_steps=10000)

f = open("win.txt", 'w')
f.write(str(win_list))
f.close()

"""## TRPO of Stable Baselines3"""

env = minesweeper_gym.MinesweeperDiscreetEnv()

model = TRPO('MlpPolicy', env, verbose=1)

# Random Agent, before training
mean_reward_before_train = evaluate(model, num_steps=2000)

# Train the agent
model.learn(total_timesteps=100000, log_interval=100)
# Save the agent
model.save("trpo_minesweeper")
del model  # delete trained model to demonstrate loading

# Load the trained agent
model = TRPO.load("trpo_minesweeper")

# Evaluate the trained agent
mean_reward = evaluate(model, num_steps=10000)

