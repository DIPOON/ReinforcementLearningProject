{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1800362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat, configure\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "#from stable_baselines3.her.her_replay_buffer import HerReplayBuffer\n",
    "\n",
    "#import minesweeper_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0f50e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce9af8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from six import StringIO\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# default : easy board\n",
    "BOARD_SIZE = 4\n",
    "NUM_MINES = 1\n",
    "\n",
    "# cell values, non-negatives indicate number of neighboring mines\n",
    "MINE = -1\n",
    "CLOSED = -2\n",
    "\n",
    "\n",
    "def board2str(board, end='\\n'):\n",
    "    \"\"\"\n",
    "    Format a board as a string\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    board : np.array\n",
    "    end : str\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    s : str\n",
    "    \"\"\"\n",
    "    s = ''\n",
    "    for x in range(board.shape[1]):\n",
    "        for y in range(board.shape[2]):\n",
    "            s += str(board[0][x][y]) + '\\t'\n",
    "        s += end\n",
    "    return s[:-len(end)]\n",
    "\n",
    "\n",
    "def is_new_move(my_board, x, y):\n",
    "    \"\"\" return true if this is not an already clicked place\"\"\"\n",
    "    return my_board[0, x, y] == CLOSED\n",
    "\n",
    "\n",
    "def is_valid(x, y):\n",
    "    \"\"\" returns if the coordinate is valid\"\"\"\n",
    "    return (x >= 0) & (x < BOARD_SIZE) & (y >= 0) & (y < BOARD_SIZE)\n",
    "\n",
    "\n",
    "def is_win(my_board):\n",
    "    \"\"\" return if the game is won \"\"\"\n",
    "    return np.count_nonzero(my_board == CLOSED) == NUM_MINES\n",
    "\n",
    "\n",
    "def is_mine(board, x, y):\n",
    "    \"\"\"return if the coordinate has a mine or not\"\"\"\n",
    "    return board[0, x, y] == MINE\n",
    "\n",
    "\n",
    "def place_mines(board_size, num_mines):\n",
    "    \"\"\"generate a board, place mines randomly\"\"\"\n",
    "    mines_placed = 0\n",
    "    board = np.zeros((1, board_size, board_size), dtype=int)\n",
    "    while mines_placed < num_mines:\n",
    "        rnd = randint(0, board_size * board_size)\n",
    "        x = int(rnd / board_size)\n",
    "        y = int(rnd % board_size)\n",
    "        if is_valid(x, y):\n",
    "            if not is_mine(board, x, y):\n",
    "                board[0, x, y] = MINE\n",
    "                mines_placed += 1\n",
    "    return board\n",
    "\n",
    "class MinesweeperDiscreetEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"ansi\", \"human\"]}\n",
    "\n",
    "    def __init__(self, board_size=BOARD_SIZE, num_mines=NUM_MINES):\n",
    "        \"\"\"\n",
    "        Create a minesweeper game.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        board_size: int     shape of the board\n",
    "            - int: the same as (int, int)\n",
    "        num_mines: int   num mines on board\n",
    "        \"\"\"\n",
    "\n",
    "        self.board_size = board_size\n",
    "        self.num_mines = num_mines\n",
    "        self.board = place_mines(board_size, num_mines)\n",
    "        self.my_board = np.ones((board_size, board_size), dtype=int) * CLOSED\n",
    "        self.num_actions = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-2, high=9,\n",
    "                                            shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
    "        self.action_space = spaces.Discrete(self.board_size*self.board_size)\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n",
    "\n",
    "    def count_neighbour_mines(self, x, y):\n",
    "        \"\"\"return number of mines in neighbour cells given an x-y coordinate\n",
    "\n",
    "            Cell -->Current Cell(row, col)\n",
    "            N -->  North(row - 1, col)\n",
    "            S -->  South(row + 1, col)\n",
    "            E -->  East(row, col + 1)\n",
    "            W -->  West(row, col - 1)\n",
    "            N.E --> North - East(row - 1, col + 1)\n",
    "            N.W --> North - West(row - 1, col - 1)\n",
    "            S.E --> South - East(row + 1, col + 1)\n",
    "            S.W --> South - West(row + 1, col - 1)\n",
    "        \"\"\"\n",
    "        neighbour_mines = 0\n",
    "        for _x in range(x - 1, x + 2):\n",
    "            for _y in range(y - 1, y + 2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if is_mine(self.board, _x, _y):\n",
    "                        neighbour_mines += 1\n",
    "        return neighbour_mines\n",
    "\n",
    "    def open_neighbour_cells(self, my_board, x, y):\n",
    "        \"\"\"return number of mines in neighbour cells given an x-y coordinate\n",
    "\n",
    "            Cell -->Current Cell(row, col)\n",
    "            N -->  North(row - 1, col)\n",
    "            S -->  South(row + 1, col)\n",
    "            E -->  East(row, col + 1)\n",
    "            W -->  West(row, col - 1)\n",
    "            N.E --> North - East(row - 1, col + 1)\n",
    "            N.W --> North - West(row - 1, col - 1)\n",
    "            S.E --> South - East(row + 1, col + 1)\n",
    "            S.W --> South - West(row + 1, col - 1)\n",
    "        \"\"\"\n",
    "        for _x in range(x-1, x+2):\n",
    "            for _y in range(y-1, y+2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if is_new_move(my_board, _x, _y):\n",
    "                        my_board[0, _x, _y] = self.count_neighbour_mines(_x, _y)\n",
    "                        if my_board[0, _x, _y] == 0:\n",
    "                            my_board = self.open_neighbour_cells(my_board, _x, _y)\n",
    "        return my_board\n",
    "\n",
    "    def get_next_state(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next state.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)   visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        game_over : (bool) true if game over\n",
    "\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        game_over = False\n",
    "        if is_mine(self.board, x, y):\n",
    "            my_board[0, x, y] = MINE\n",
    "            game_over = True\n",
    "        else:\n",
    "            my_board[0, x, y] = self.count_neighbour_mines(x, y)\n",
    "            if my_board[0, x, y] == 0:\n",
    "                my_board = self.open_neighbour_cells(my_board, x, y)\n",
    "        self.my_board = my_board\n",
    "        return my_board, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset a new game episode. See gym.Env.reset()\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array, int)    next board\n",
    "        \"\"\"\n",
    "        self.my_board = np.ones((1, self.board_size, self.board_size), dtype=int) * CLOSED\n",
    "        self.board = place_mines(self.board_size, self.num_mines)\n",
    "        self.num_actions = 0\n",
    "        self.valid_actions = np.ones((self.board_size * self.board_size), dtype=bool)\n",
    "        self.win_or_lose = None\n",
    "        return self.my_board\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        See gym.Env.step().\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        action : np.array    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next board\n",
    "        reward : float        the reward for action\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}             {'valid_actions': valid_actions} - a binary vector,\n",
    "                                where false cells' values are already known to observer\n",
    "        \"\"\"\n",
    "        state = self.my_board\n",
    "        x = int(action / self.board_size)\n",
    "        y = int(action % self.board_size)\n",
    "\n",
    "        # test valid action - uncomment this part to test your action filter if needed\n",
    "        # if bool(self.valid_actions[action]) is False:\n",
    "        #    raise Exception(\"Invalid action was selected! Action Filter: {}, \"\n",
    "        #                    \"action taken: {}\".format(self.valid_actions, action))\n",
    "        reward = 0\n",
    "        while reward==0:\n",
    "          next_state, reward, done, info = self.next_step(state, x, y)\n",
    "        self.my_board = next_state\n",
    "        self.num_actions += 1\n",
    "        #self.valid_actions = (next_state.flatten() == CLOSED)\n",
    "        self.valid_actions = (self.my_board == -2).reshape(self.valid_actions.shape)\n",
    "        info['valid_actions'] = self.valid_actions\n",
    "        info['num_actions'] = self.num_actions\n",
    "        self.win_or_lose = info['is_success']\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def is_guess(self, my_board, x, y):\n",
    "        for _x in range(x-1, x+2):\n",
    "            for _y in range(y-1, y+2):\n",
    "                if is_valid(_x, _y):\n",
    "                    if not is_new_move(my_board, _x, _y):\n",
    "                        if (x != _x) or (y != _y):\n",
    "                            return False\n",
    "        return True\n",
    "                    \n",
    "\n",
    "    def next_step(self, state, x, y):\n",
    "        \"\"\"\n",
    "        Get the next observation, reward, done, and info.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        state : (np.array)    visible board\n",
    "        x : int    location\n",
    "        y : int    location\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "        next_state : (np.array)    next visible board\n",
    "        reward : float               the reward\n",
    "        done : bool           whether the game end or not\n",
    "        info : {}\n",
    "        \"\"\"\n",
    "        my_board = state\n",
    "        win_or_lose = False\n",
    "        reward = 0\n",
    "        done = False\n",
    "        t_b = False\n",
    "        if not is_new_move(my_board, x, y):\n",
    "            return my_board, -0.3, False, {'is_success': win_or_lose}\n",
    "        elif self.is_guess(my_board, x, y): # if guess\n",
    "            t_b = True\n",
    "\n",
    "        state, game_over = self.get_next_state(my_board, x, y)\n",
    "        if game_over:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            #return state, -100, True, {}\n",
    "        elif is_win(state):\n",
    "            reward = 1\n",
    "            done = True\n",
    "            win_or_lose = True\n",
    "            #return state, 1000, True, {}\n",
    "        elif t_b: # if guess\n",
    "            # random x, y\n",
    "            reward = -0.3\n",
    "        else: # progress\n",
    "            reward = 0.9\n",
    "            #return state, 0, False, {}\n",
    "            \n",
    "        return state, reward, done, {'is_success': win_or_lose}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        See gym.Env.render().\n",
    "        \"\"\"\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "        s = board2str(self.my_board)\n",
    "        outfile.write(s)\n",
    "        if mode != 'human':\n",
    "            return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "366c49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_steps=1000):\n",
    "  \"\"\"\n",
    "  Evaluate a RL agent\n",
    "  :param model: (BaseRLModel object) the RL Agent\n",
    "  :param num_steps: (int) number of timesteps to evaluate it\n",
    "  :return: (float) Mean reward for the last 100 episodes\n",
    "  \"\"\"\n",
    "  episode_rewards = [0.0]\n",
    "  obs = env.reset()\n",
    "  n_games = 0\n",
    "  n_win = 0\n",
    "  \n",
    "  for i in range(num_steps):\n",
    "      # _states are only useful when using LSTM policies\n",
    "      action, _states = model.predict(obs)\n",
    "\n",
    "      obs, reward, done, info = env.step(action)\n",
    "      \n",
    "      # Stats\n",
    "      episode_rewards[-1] += reward\n",
    "      if done:\n",
    "          n_games += 1\n",
    "          if env.win_or_lose:\n",
    "              n_win += 1\n",
    "          obs = env.reset()\n",
    "          episode_rewards.append(0.0)\n",
    "  # Compute mean reward for the last 100 episodes\n",
    "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
    "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards), \"Win rate:\", round(n_win/n_games, 3))\n",
    "  \n",
    "  return mean_100ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd228f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.sample()[None].shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same'),\n",
    "#             nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6096f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    lr0 = initial_value\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining: = 1.0 - (num_timesteps / total_timesteps)\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        #if progress_remaining > 0.5:\n",
    "        #    return initial_value\n",
    "        #else:\n",
    "        #    return progress_remaining * initial_value * 2\n",
    "        #return progress_remaining * initial_value\n",
    "        nonlocal lr0\n",
    "        lr0 = max(0.001, lr0 * 0.99975) # 0.99975\n",
    "        return lr0\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d339dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining: 1 - num_step / total_step\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        if progress_remaining > 0.5:\n",
    "            return initial_value\n",
    "        else:\n",
    "            return progress_remaining * initial_value *2\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1838fe93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_515384/2364046332.py:94: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  shape=(1, self.board_size, self.board_size), dtype=np.int)\n",
      "/tmp/ipykernel_515384/2364046332.py:96: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.valid_actions = np.ones((self.board_size * self.board_size), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "env = MinesweeperDiscreetEnv(4,1)\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")\n",
    "model = QRDQN('CnnPolicy', env, \n",
    "            #learning_rate=1e-3, \n",
    "            #policy_kwargs=dict(activation_fn=nn.ReLU,\n",
    "            #                   net_arch=[256, 256, 256]), \n",
    "            policy_kwargs=policy_kwargs,\n",
    "            learning_rate=linear_schedule(0.001),\n",
    "            #batch_size=1, \n",
    "            gamma=0.1, \n",
    "            #tau=0.8,\n",
    "            train_freq=(5, 'episode'),\n",
    "            target_update_interval=100,\n",
    "            learning_starts=1,\n",
    "            #buffer_size=4,\n",
    "            exploration_fraction=0.9, \n",
    "            exploration_initial_eps=0.95, \n",
    "            exploration_final_eps=0.01,\n",
    "            tensorboard_log=\"./qrdqn_tensorboard/\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90bba517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.qrdqn.qrdqn.QRDQN at 0x7fd7d067a820>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#callback = TensorboardCallback(eval_env=MinesweeperDiscreetEnv())\n",
    "model.learn(total_timesteps=int(2e5), log_interval=10, reset_num_timesteps=True, tb_log_name = 'QRDQN_s4m1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "80c62066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"QRDQN_s4m1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64987ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
