{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069a5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MinesweeperEnv(object):\n",
    "    def __init__(self, width, height, n_mines,\n",
    "        # based on https://github.com/jakejhansen/minesweeper_solver\n",
    "        #rewards={'win':1, 'lose':-1, 'progress':0.3, 'guess':-0.3, 'no_progress' : -0.3}):\n",
    "        rewards={'win':1, 'lose':0, 'bomb':-0.5, 'progress':0.3, 'guess':-0.3, 'no_progress' : -0.3}):\n",
    "        self.nrows, self.ncols = width, height\n",
    "        self.ntiles = self.nrows * self.ncols\n",
    "        self.n_mines = n_mines\n",
    "        self.grid = self.init_grid()\n",
    "        self.board = self.get_board()\n",
    "        self.state, self.state_im = self.init_state()\n",
    "        self.n_clicks = 0\n",
    "        self.n_progress = 0\n",
    "        self.n_wins = 0\n",
    "\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def init_grid(self):\n",
    "        board = np.zeros((self.nrows, self.ncols), dtype='object')\n",
    "        mines = self.n_mines\n",
    "\n",
    "        while mines > 0:\n",
    "            row, col = random.randint(0, self.nrows-1), random.randint(0, self.ncols-1)\n",
    "            if board[row][col] != 'B':\n",
    "                board[row][col] = 'B'\n",
    "                mines -= 1\n",
    "\n",
    "        return board\n",
    "\n",
    "    def get_neighbors(self, coord):\n",
    "        x,y = coord[0], coord[1]\n",
    "\n",
    "        neighbors = []\n",
    "        for col in range(y-1, y+2):\n",
    "            for row in range(x-1, x+2):\n",
    "                if ((x != row or y != col) and\n",
    "                    (0 <= col < self.ncols) and\n",
    "                    (0 <= row < self.nrows)):\n",
    "                    neighbors.append(self.grid[row,col])\n",
    "\n",
    "        return np.array(neighbors)\n",
    "\n",
    "    def is_guess(self, state, coord):\n",
    "        x,y = coord[0], coord[1]\n",
    "\n",
    "        neighbors = []\n",
    "        for col in range(y-1, y+2):\n",
    "            for row in range(x-1, x+2):\n",
    "                if ((x != row or y != col) and\n",
    "                    (0 <= col < self.ncols) and\n",
    "                    (0 <= row < self.nrows)):\n",
    "                    neighbors.append(state[row, col])\n",
    "        neighbors = np.array(neighbors)\n",
    "        return all(t==-0.125 for t in neighbors)\n",
    "\n",
    "    def count_bombs(self, coord):\n",
    "        neighbors = self.get_neighbors(coord)\n",
    "        return np.sum(neighbors=='B')\n",
    "\n",
    "    def possibility_bombs(self, coord):\n",
    "        neighbors = self.get_neighbors(coord)\n",
    "        rand = np.random.random() # random value b/w 0 & 1\n",
    "        if np.sum(neighbors=='B') == 0:\n",
    "            if rand < 0.1: # possibility of mine\n",
    "                ret = 1\n",
    "            else:\n",
    "                ret = 0\n",
    "        else:\n",
    "            ret = 1\n",
    "        return ret\n",
    "\n",
    "    def get_board(self):\n",
    "        '''\n",
    "        This board is revised.\n",
    "        Number 1 means there is a mine around or more.\n",
    "        Number 0 means there are no mines around.\n",
    "        '''\n",
    "        board = self.grid.copy()\n",
    "\n",
    "        coords = []\n",
    "        for x in range(self.nrows):\n",
    "            for y in range(self.ncols):\n",
    "                if self.grid[x,y] != 'B':\n",
    "                    coords.append((x,y))\n",
    "\n",
    "        for coord in coords:\n",
    "            #board[coord] = self.count_bombs(coord)\n",
    "            board[coord] = self.possibility_bombs(coord)\n",
    "\n",
    "        return board\n",
    "\n",
    "    def get_state_im(self, state):\n",
    "        '''\n",
    "        Gets the numeric image representation state of the board.\n",
    "        This is what will be the input for the DQN.\n",
    "        \n",
    "        Meanings: 'U': Unknown i.e. not opened\n",
    "                  'B': Mine\n",
    "        '''\n",
    "\n",
    "        state_im = [t['value'] for t in state]\n",
    "        state_im = np.reshape(state_im, (self.nrows, self.ncols, 1)).astype(object)\n",
    "\n",
    "        state_im[state_im=='U'] = -1\n",
    "        state_im[state_im=='B'] = -2\n",
    "\n",
    "        state_im = state_im.astype(np.int8) / 8\n",
    "        state_im = state_im.astype(np.float16)\n",
    "\n",
    "        return state_im\n",
    "\n",
    "    def init_state(self):\n",
    "        unsolved_array = np.full((self.nrows, self.ncols), 'U', dtype='object')\n",
    "\n",
    "        state = []\n",
    "        for (x, y), value in np.ndenumerate(unsolved_array):\n",
    "            state.append({'coord': (x, y), 'value':value})\n",
    "\n",
    "        state_im = self.get_state_im(state)\n",
    "\n",
    "        return state, state_im\n",
    "\n",
    "    def color_state(self, value):\n",
    "        if value == -1:\n",
    "            color = 'white'\n",
    "        elif value == 0:\n",
    "            color = 'slategrey'\n",
    "        elif value == 1:\n",
    "            color = 'blue'\n",
    "        elif value == 2:\n",
    "            color = 'green'\n",
    "        elif value == 3:\n",
    "            color = 'red'\n",
    "        elif value == 4:\n",
    "            color = 'midnightblue'\n",
    "        elif value == 5:\n",
    "            color = 'brown'\n",
    "        elif value == 6:\n",
    "            color = 'aquamarine'\n",
    "        elif value == 7:\n",
    "            color = 'black'\n",
    "        elif value == 8:\n",
    "            color = 'silver'\n",
    "        else:\n",
    "            color = 'magenta'\n",
    "\n",
    "        return f'color: {color}'\n",
    "\n",
    "    def draw_state(self, state_im):\n",
    "        state = state_im * 8.0\n",
    "        state_df = pd.DataFrame(state.reshape((self.nrows, self.ncols)), dtype=np.int8)\n",
    "\n",
    "        display(state_df.style.applymap(self.color_state))\n",
    "\n",
    "    def click(self, action_index):\n",
    "        coord = self.state[action_index]['coord']\n",
    "        value = self.board[coord]\n",
    "\n",
    "        \"\"\"\n",
    "        # ensure first move is not a bomb\n",
    "        if (value == 'B') and (self.n_clicks == 0):\n",
    "            grid = self.grid.reshape(1, self.ntiles)\n",
    "            move = np.random.choice(np.nonzero(grid!='B')[1])\n",
    "            coord = self.state[move]['coord']\n",
    "            value = self.board[coord]\n",
    "            self.state[move]['value'] = value\n",
    "        else:\n",
    "            # make state equal to board at given coordinates\n",
    "            self.state[action_index]['value'] = value\n",
    "        \"\"\"\n",
    "        # make state equal to board at given coordinates\n",
    "        self.state[action_index]['value'] = value\n",
    "\n",
    "        \"\"\"\n",
    "        # reveal all neighbors if value is 0\n",
    "        if value == 0.0:\n",
    "            self.reveal_neighbors(coord, clicked_tiles=[])\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_clicks += 1\n",
    "\n",
    "    def reveal_neighbors(self, coord, clicked_tiles):\n",
    "        processed = clicked_tiles\n",
    "        state_df = pd.DataFrame(self.state)\n",
    "        x,y = coord[0], coord[1]\n",
    "\n",
    "        neighbors = []\n",
    "        for col in range(y-1, y+2):\n",
    "            for row in range(x-1, x+2):\n",
    "                if ((x != row or y != col) and\n",
    "                    (0 <= col < self.ncols) and\n",
    "                    (0 <= row < self.nrows) and\n",
    "                    ((row, col) not in processed)):\n",
    "\n",
    "                    # prevent redundancy for adjacent zeros\n",
    "                    processed.append((row,col))\n",
    "\n",
    "                    index = state_df.index[state_df['coord'] == (row,col)].tolist()[0]\n",
    "\n",
    "                    self.state[index]['value'] = self.board[row, col]\n",
    "\n",
    "                    # recursion in case neighbors are also 0\n",
    "                    if self.board[row, col] == 0.0:\n",
    "                        self.reveal_neighbors((row, col), clicked_tiles=processed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_clicks = 0\n",
    "        self.n_progress = 0\n",
    "        self.grid = self.init_grid()\n",
    "        self.board = self.get_board()\n",
    "        self.state, self.state_im = self.init_state()\n",
    "\n",
    "    def step(self, action_index):\n",
    "        done = False\n",
    "        coords = self.state[action_index]['coord']\n",
    "\n",
    "        current_state = self.state_im\n",
    "\n",
    "        # get neighbors before action\n",
    "        is_guess_b = self.is_guess(current_state, coords)\n",
    "\n",
    "        self.click(action_index)\n",
    "\n",
    "        # update state image\n",
    "        new_state_im = self.get_state_im(self.state)\n",
    "        self.state_im = new_state_im\n",
    "\n",
    "        board_flatten = self.board.flatten()\n",
    "        unsolved_index = np.where(self.state_im.flatten() == -0.125)\n",
    "        if np.array_equal(np.where(board_flatten == 'B'), \n",
    "                            unsolved_index): # if win\n",
    "            #elif np.sum(new_state_im==-0.125) == self.n_mines: # if win\n",
    "            reward = self.rewards['win']\n",
    "            done = True\n",
    "            self.n_progress += 1\n",
    "            self.n_wins += 1\n",
    "        \n",
    "        elif (np.sum(self.state_im == -0.125) == 0) or \\\n",
    "                (np.sum(board_flatten[unsolved_index] != 'B') == 0): # if lose\n",
    "            reward = self.rewards['lose']\n",
    "            done = True\n",
    "\n",
    "        # Does not lose if detect mine\n",
    "        elif self.state[action_index]['value']=='B': # if find mines\n",
    "            #reward = self.rewards['lose']\n",
    "            reward = self.rewards['bomb']\n",
    "            #done = True\n",
    "        \n",
    "        elif np.sum(self.state_im == -0.125) == np.sum(current_state == -0.125):\n",
    "            reward = self.rewards['no_progress']\n",
    "\n",
    "        else: # if progress\n",
    "            if is_guess_b: # if guess (all neighbors are unsolved)\n",
    "                reward = self.rewards['guess']\n",
    "\n",
    "            else:\n",
    "                reward = self.rewards['progress']\n",
    "                self.n_progress += 1 # track n of non-isoloated clicks\n",
    "\n",
    "        return self.state_im, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6704aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhde1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#Base code was written by Jonas Busk - Modified to suit project by Jacob Jon Hansen\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "import gym\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469783cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateConverter(OUT, state):\n",
    "        \"\"\" Converts 2d state to one-hot encoded 3d state\n",
    "            input: state (rows x cols)\n",
    "            output: state3d (row x cols x 10) (if full)\n",
    "                            (row x cols x 2) (if condensed)\n",
    "                            (row x cols x 1) (if image)\n",
    "\n",
    "        \"\"\"\n",
    "        rows, cols = state.shape\n",
    "        if OUT == \"FULL\":\n",
    "            res = np.zeros((rows,cols,10), dtype = int)\n",
    "            for i in range(0,8):\n",
    "                res[:,:,i] = state == i+1 #1-7\n",
    "            res[:,:,8] = state == 'U'\n",
    "            res[:,:,9] = state == 'E'\n",
    "           \n",
    "            return(res)\n",
    "        elif OUT == \"CONDENSED\":\n",
    "            #Outputs a condensed representation of nxmx2\n",
    "            #First layer is the value of the intergers\n",
    "            #Second layer is true if field is empty, 0 otherwise\n",
    "\n",
    "            res = np.zeros((rows, cols, 2))\n",
    "            filtr = ~np.logical_or(state == \"U\", state == \"B\") #Not U or E\n",
    "            res[filtr,0] = state[filtr] / 4\n",
    "            res[state == \"U\", 1] = 1\n",
    "            return(res)\n",
    "\n",
    "        elif OUT == \"IMAGE\":\n",
    "            #Outputs an image\n",
    "            res = np.zeros((rows, cols,1))\n",
    "            res[state == \"U\", 0] = -1\n",
    "            res[state == \"E\", 0] = 0\n",
    "            filtr = ~np.logical_or(state == \"U\", state == \"E\") #Not U or E\n",
    "            res[filtr, 0] = state[filtr] / 8\n",
    "            return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb2b69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_state(state):\n",
    "  n = int(math.sqrt(len(state)))\n",
    "  new_state = np.zeros((n,n), dtype=object)\n",
    "  for s in state:\n",
    "    x, y = s['coord']\n",
    "    value = s['value']\n",
    "    new_state[x][y] = value\n",
    "  return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62df88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_pl: (?, 72)\n",
      "actions_pl: (?, 2)\n",
      "advantages_pl: (?,)\n",
      "l_hidden: (?, 288)\n",
      "l_hidden2: (?, 220)\n",
      "l_hidden3: (?, 220)\n",
      "l_out: (?, 36)\n",
      "[[0.02701008 0.02707972 0.02682858 0.02921804 0.02788166 0.02839976\n",
      "  0.02749893 0.02766426 0.02806639 0.02761758 0.02899935 0.02750686\n",
      "  0.02920808 0.02873861 0.0275104  0.02811758 0.02777183 0.02884548\n",
      "  0.02626789 0.0289324  0.02742492 0.02603118 0.02705365 0.02793705\n",
      "  0.02830545 0.02873754 0.02436297 0.02656362 0.02752407 0.02791508\n",
      "  0.03058466 0.02737801 0.02673968 0.02865472 0.02759228 0.02803159]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8f913ae6e786>:66: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.sum(neighbors=='B') == 0:\n"
     ]
    }
   ],
   "source": [
    "#Base code was written by Jonas Busk - Modified to suit project by Jacob Jon Hansen\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "import gym\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "# from minesweeper_tk import Minesweeper\n",
    "\n",
    "\n",
    "model = \"condensed_6x6_CNN\"\n",
    "# training settings\n",
    "\n",
    "epochs = 500000 # number of training batches\n",
    "batch_size = 200 # number of timesteps in a batch\n",
    "rollout_limit = 50 # max rollout length\n",
    "discount_factor = 0 # reward discount factor (gamma), 1.0 = no discount\n",
    "learning_rate = 0.000002  # you know this by now #0.001, \n",
    "                                               #5600: 78% win --> LR: 0.0001\n",
    "                                               #6801: 87% win --> LR: 0.00002\n",
    "                                                \n",
    "early_stop_loss = 0 # stop training if loss < early_stop_loss, 0 or False to disable\n",
    "\n",
    "\"\"\" condensed\n",
    "epochs = 100000 # number of training batches\n",
    "batch_size = 400 # number of timesteps in a batch\n",
    "rollout_limit = 50 # max rollout length\n",
    "discount_factor = 0 # reward discount factor (gamma), 1.0 = no discount\n",
    "learning_rate = 0.00004  # you know this by now #0.0005\n",
    "early_stop_loss = 0 # stop training if loss < early_stop_loss, 0 or False to disable\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" 261 epocs to learn 2 specific board (overfit)\n",
    "epochs = 10000 # number of training batches\n",
    "batch_size = 200 # number of timesteps in a batch\n",
    "rollout_limit = 130 # max rollout length\n",
    "discount_factor = 0 # reward discount factor (gamma), 1.0 = no discount\n",
    "learning_rate = 0.001 # you know this by now\n",
    "early_stop_loss = 0 # stop training if loss < early_stop_loss, 0 or False to disable\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# setup policy network\n",
    "n = 6\n",
    "n_inputs = 6*6*2\n",
    "n_hidden = 6*6*8\n",
    "n_hidden2 = 220\n",
    "n_hidden3 = 220\n",
    "n_hidden4 = 220\n",
    "n_outputs = 6*6\n",
    "\n",
    "dropout = 0.25\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "states_pl = tf.placeholder(tf.float32, [None, n_inputs], name='states_pl')\n",
    "actions_pl = tf.placeholder(tf.int32, [None, 2], name='actions_pl')\n",
    "advantages_pl = tf.placeholder(tf.float32, [None], name='advantages_pl')\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_pl')\n",
    "\n",
    "input_layer = tf.reshape(states_pl, [-1, n, n, 2])\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer,filters=18,kernel_size=[5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "conv2 = tf.layers.conv2d(inputs=conv1,filters=36,kernel_size=[3, 3],padding=\"same\", activation=tf.nn.relu)\n",
    "conv2_flat = tf.layers.flatten(conv2)\n",
    "l_hidden = tf.layers.dense(inputs=conv2_flat, units=n_hidden, activation=relu, name='l_hidden')\n",
    "l_hidden2 = tf.layers.dense(inputs=l_hidden, units=n_hidden2, activation=relu, name='l_hidden2')\n",
    "l_hidden3 = tf.layers.dense(inputs=l_hidden2, units=n_hidden3, activation=relu, name='l_hidden3')\n",
    "l_out = tf.layers.dense(inputs=l_hidden3, units=n_outputs, activation=softmax, name='l_out')\n",
    "\n",
    "# print network\n",
    "print('states_pl:', states_pl.get_shape())\n",
    "print('actions_pl:', actions_pl.get_shape())\n",
    "print('advantages_pl:', advantages_pl.get_shape())\n",
    "print('l_hidden:', l_hidden.get_shape())\n",
    "print('l_hidden2:', l_hidden2.get_shape())\n",
    "print('l_hidden3:', l_hidden3.get_shape())\n",
    "print('l_out:', l_out.get_shape())\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_f = -tf.reduce_mean(tf.multiply(tf.log(tf.gather_nd(l_out, actions_pl)), advantages_pl))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_pl, beta1=0.8, beta2=0.92)\n",
    "train_f = optimizer.minimize(loss_f)\n",
    "\n",
    "saver = tf.train.Saver() # we use this later to save the model\n",
    "\n",
    "# test forward pass\n",
    "env = MinesweeperEnv(6, 6, 6)\n",
    "OUT = \"CONDENSED\"\n",
    "\n",
    "state = stateConverter(OUT, get_state(env.state)).flatten()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    action_probabilities = sess.run(fetches=l_out, feed_dict={states_pl: [state]})\n",
    "print(action_probabilities)\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def get_rollout(sess, env, rollout_limit=None, stochastic=False, seed=None):\n",
    "    \"\"\"Generate rollout by iteratively evaluating the current policy on the environment.\"\"\"\n",
    "    rollout_limit = rollout_limit\n",
    "    \n",
    "    env.reset()\n",
    "    s = stateConverter(OUT, get_state(env.state)).flatten()\n",
    "    states, actions, rewards = [], [], []\n",
    "    for i in range(rollout_limit):\n",
    "        a = get_action(sess, s, stochastic)\n",
    "        s1, r, done = env.step(a)\n",
    "        s1 = stateConverter(OUT, get_state(env.state))\n",
    "        s1 = s1.flatten()\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = s1\n",
    "        if done: break\n",
    "    return states, actions, rewards, i+1\n",
    "\n",
    "def get_action(sess, state, stochastic=False):\n",
    "    \"\"\"Choose an action, given a state, with the current policy network.\"\"\"\n",
    "    a_prob = sess.run(fetches=l_out, feed_dict={states_pl: np.atleast_2d(state)})\n",
    "\n",
    "    if stochastic:\n",
    "        # sample action from distribution\n",
    "        return (np.cumsum(np.asarray(a_prob)) > np.random.rand()).argmax()\n",
    "    else:\n",
    "        # select action with highest probability\n",
    "        return a_prob.argmax()\n",
    "\n",
    "def get_advantages(rewards, rollout_limit, discount_factor, eps=1e-12):\n",
    "    \"\"\"Compute advantages\"\"\"\n",
    "    returns = get_returns(rewards, rollout_limit, discount_factor)\n",
    "    # standardize columns of returns to get advantages\n",
    "    advantages = (returns - np.mean(returns, axis=0)) / (np.std(returns, axis=0) + eps)\n",
    "    # restore original rollout lengths\n",
    "    advantages = [adv[:len(rewards[i])] for i, adv in enumerate(advantages)]\n",
    "    return advantages\n",
    "\n",
    "def get_returns(rewards, rollout_limit, discount_factor):\n",
    "    \"\"\"Compute the cumulative discounted rewards, a.k.a. returns.\"\"\"\n",
    "    returns = np.zeros((len(rewards), rollout_limit))\n",
    "    for i, r in enumerate(rewards):\n",
    "        returns[i, len(r) - 1] = r[-1]\n",
    "        for j in reversed(range(len(r)-1)):\n",
    "            returns[i,j] = r[j] + discount_factor * returns[i,j+1]\n",
    "    return returns\n",
    "\n",
    "def get_winrate(sess, env):\n",
    "    games = 0\n",
    "    moves = 0\n",
    "    stuck = 0\n",
    "    won_games = 0\n",
    "    lost_games = 0\n",
    "    r = 0\n",
    "    while games < 1000:\n",
    "        while True:\n",
    "            s = stateConverter(OUT, get_state(env.state)).flatten()\n",
    "            if r < 0: \n",
    "                a = get_action(sess, s, stochastic=True)\n",
    "            else:\n",
    "                a = get_action(sess, s, stochastic=False)\n",
    "            moves += 1\n",
    "            s, r, done= env.step(a)\n",
    "            s = get_state(env.state)\n",
    "            s = s.flatten()\n",
    "            if r == 1:\n",
    "                won_games += 1\n",
    "            if r == 0:\n",
    "                lost_games += 1\n",
    "\n",
    "            if done:\n",
    "                games += 1\n",
    "                env.reset()\n",
    "                moves = 0\n",
    "                break\n",
    "            elif moves >= 100:\n",
    "                stuck += 1\n",
    "                games += 1\n",
    "                lost_games += 1\n",
    "                env.reset()\n",
    "                moves = 0\n",
    "                break\n",
    "    return(won_games/games)\n",
    "\n",
    "def smooth(y,factor):\n",
    "    if type(y)!=list:\n",
    "        y = list(y)\n",
    "    return pd.Series(y).rolling(window=factor).mean()#[factor:]\n",
    "# train policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8268ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from condensed_6x6_CNN/condensed_6x6_CNN.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8f913ae6e786>:66: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.sum(neighbors=='B') == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    stats = pickle.load(open(\"{}/stats.p\".format(model), \"rb\"))\n",
    "    saver.restore(sess, \"{}/{}.ckpt\".format(model,model))\n",
    "    \n",
    "    win_rate = get_winrate(sess, env)\n",
    "    print(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
