{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN의 개량.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Minesweeper solver using DQN\n","environment from [github.com/sdlee94/Minesweeper-AI-Reinforcement-Learning](https://github.com/sdlee94/Minesweeper-AI-Reinforcement-Learning)"],"metadata":{"id":"PSlHwdF77ftq"}},{"cell_type":"markdown","source":["일단 수렴성 확인하려고"],"metadata":{"id":"SmQF4mrTYx45"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua0Y13KfNp_c","executionInfo":{"status":"ok","timestamp":1653488049590,"user_tz":-540,"elapsed":17954,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}},"outputId":"46f8a31c-e464-4863-b1b2-c02923a2d70c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[Errno 2] No such file or directory: '/content/drive/My Drive/2022-1 RL/Project'\n","/content\n","TensorFlow 1.x selected.\n"]}],"source":["import sys, os\n","\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    sys.path.append('/content/drive/My Drive/RL_Project')\n","    %cd /content/drive/My Drive/2022-1 RL/Project\n","\n","%tensorflow_version 1.x"]},{"cell_type":"code","source":["!pip install h5py==2.10.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZZmoWBKbL1m","executionInfo":{"status":"ok","timestamp":1653488054062,"user_tz":-540,"elapsed":4475,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}},"outputId":"992bcbe8-4ba8-4b94-c244-901a19243bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.6)\n","Installing collected packages: h5py\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n"]}]},{"cell_type":"code","source":["import random\n","import pickle\n","from tqdm import tqdm\n","import warnings\n","from collections import deque\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Conv2D, Dense, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import TensorBoard\n","\n","from minesweeper_env import MinesweeperEnv"],"metadata":{"id":"KD-M8uoH1xx-","executionInfo":{"status":"ok","timestamp":1653488057304,"user_tz":-540,"elapsed":3246,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66d8c9c7-8d18-4d66-a41e-e51dfd74ad4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","source":["warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","# Environment settings\n","MEM_SIZE = 50_000 # number of moves to store in replay buffer\n","MEM_SIZE_MIN = 1_000 # min number of moves in replay buffer\n","\n","# Learning settings\n","BATCH_SIZE = 64\n","learn_rate = 0.01\n","LEARN_DECAY = 0.99975\n","LEARN_MIN = 0.001\n","DISCOUNT = 0.1 #gamma\n","\n","# Exploration settings\n","epsilon = 0.95\n","EPSILON_DECAY = 0.99975\n","EPSILON_MIN = 0.01\n","\n","# DQN settings \n","CONV_UNITS = 256 # number of neurons in each conv layer # 여기 수정했다.\n","DENSE_UNITS = 512 # number of neurons in fully connected dense layer\n","UPDATE_TARGET_EVERY = 5\n","\n","AGG_STATS_EVERY = 100 # calculate stats every 100 games for tensorboard\n","SAVE_MODEL_EVERY = 10_000 # save model and replay every 10,000 episodes"],"metadata":{"id":"_eUCzIS4A219"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beginner: (9x9, 10) / Intermediate: (16x16, 40) / Expert: (16x30, 99)\n","width, height, n_mines = 9, 9, 10\n","\n","# rewards = {'win': 1, 'lose': -1, 'progress': 0.3, 'guess': -0.3, 'no_progress': -0.3}\n","env = MinesweeperEnv(width, height, n_mines)\n","\n","progress_list, wins_list, ep_rewards = [], [], []\n","n_clicks = 0\n","episodes = 100_000"],"metadata":{"id":"F9VB3nNOY30P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ModifiedTensorBoard(TensorBoard):\n","\n","    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.step = 1\n","        self.writer = tf.summary.FileWriter(self.log_dir)\n","\n","    # Overriding this method to stop creating default log writer\n","    def set_model(self, model):\n","        pass\n","\n","    # Overrided, saves logs with our step number\n","    # (otherwise every .fit() will start writing from 0th step)\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.update_stats(**logs)\n","\n","    # Overrided\n","    # We train for one batch only, no need to save anything at epoch end\n","    def on_batch_end(self, batch, logs=None):\n","        pass\n","\n","    # Overrided, so won't close writer\n","    def on_train_end(self, _):\n","        pass\n","\n","    # Custom method for saving own metrics\n","    # Creates writer, writes custom metrics and closes writer\n","    def update_stats(self, **stats):\n","        self._write_logs(stats, self.step)"],"metadata":{"id":"WqSI2EG0EC3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dqn(learn_rate, input_dims, n_actions, conv_units, dense_units):\n","    model = Sequential([\n","                        Conv2D(conv_units, (3, 3), activation='relu', padding='same'), # input 제한 없앰\n","                        Conv2D(conv_units, (3, 3), activation='relu', padding='same'),\n","                        Conv2D(conv_units, (3, 3), activation='relu', padding='same'),\n","                        Conv2D(conv_units, (3, 3), activation='relu', padding='same'),\n","                        Flatten(),\n","                        Dense(dense_units, activation='relu'),\n","                        Dense(dense_units, activation='relu'),\n","                        Dense(n_actions, activation='linear'),\n","    ])\n","    model.compile(optimizer=Adam(lr=learn_rate, epsilon=1e-4), loss='mse')\n","    return model"],"metadata":{"id":"637gpjxM_71d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DQNAgent(object):\n","    def __init__(self, env, conv_units=64, dense_units=256):\n","        self.env = env\n","\n","        # Deep Q-learning Parameters\n","        self.discount = DISCOUNT\n","        self.learn_rate = learn_rate\n","        self.epsilon = epsilon\n","        self.model = dqn(\n","            self.learn_rate, self.env.state_im.shape, self.env.ntiles, conv_units, dense_units)\n","\n","        # target model - this is what we predict against every step\n","        self.target_model = dqn(\n","            self.learn_rate, self.env.state_im.shape, self.env.ntiles, conv_units, dense_units)\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","        self.replay_memory = deque(maxlen=MEM_SIZE)\n","        self.target_update_counter = 0\n","\n","        self.tensorboard = ModifiedTensorBoard(log_dir=f'logs/DQN')\n","\n","    def get_action(self, state):\n","        board = state.reshape(1, self.env.ntiles)\n","        unsolved = [i for i, x in enumerate(board[0]) if x==-0.125]\n","\n","        rand = np.random.random() # random value b/w 0 & 1\n","\n","        if rand < self.epsilon: # random move (explore)\n","            move = np.random.choice(unsolved)\n","        else:\n","            moves = self.model.predict(np.reshape(state, (1, self.env.nrows, self.env.ncols, 1)))\n","            moves[board!=-0.125] = np.min(moves) # set already clicked tiles to min value\n","            move = np.argmax(moves)\n","\n","        return move\n","\n","    def update_replay_memory(self, transition):\n","        self.replay_memory.append(transition)\n","\n","    def train(self, done):\n","        if len(self.replay_memory) < MEM_SIZE_MIN:\n","            return\n","\n","        batch = random.sample(self.replay_memory, BATCH_SIZE)\n","\n","        current_states = np.array([transition[0] for transition in batch])\n","        current_qs_list = self.model.predict(current_states)\n","\n","        new_current_states = np.array([transition[3] for transition in batch])\n","        future_qs_list = self.target_model.predict(new_current_states)\n","\n","        X,y = [], []\n","\n","        for i, (current_state, action, reward, new_current_state, done) in enumerate(batch):\n","            if not done:\n","                max_future_q = np.max(future_qs_list[i])\n","                new_q = reward + DISCOUNT * max_future_q\n","            else:\n","                new_q = reward\n","\n","            current_qs = current_qs_list[i]\n","            current_qs[action] = new_q\n","\n","            X.append(current_state)\n","            y.append(current_qs)\n","\n","        self.model.fit(np.array(X),\n","                       np.array(y),\n","                       batch_size=BATCH_SIZE,\n","                       shuffle=False,\n","                       verbose=0,\n","                       callbacks=[self.tensorboard] if done else None)\n","\n","        # updating to determine if we want to update target_model yet\n","        if done:\n","            self.target_update_counter += 1\n","\n","        if self.target_update_counter > UPDATE_TARGET_EVERY:\n","            self.target_model.set_weights(self.model.get_weights())\n","            self.target_update_counter = 0\n","\n","        # decay learn_rate\n","        self.learn_rate = max(LEARN_MIN, self.learn_rate*LEARN_DECAY)\n","\n","        # decay epsilon\n","        self.epsilon = max(EPSILON_MIN, self.epsilon*EPSILON_DECAY)"],"metadata":{"id":"uRiVd5ZAcbI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent = DQNAgent(env, conv_units=CONV_UNITS, dense_units=DENSE_UNITS)\n","agent.model = tf.keras.models.load_model(f'/content/drive/My Drive/RL_Project/models/dqn.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0-qPjBZZGVT","executionInfo":{"status":"ok","timestamp":1653488066452,"user_tz":-540,"elapsed":8642,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}},"outputId":"5b928a27-6d11-4cec-e463-f3293001cf28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"]}]},{"cell_type":"code","source":["# 모델 구조를 확인합니다\n","agent.model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5NE3lSmGZscR","executionInfo":{"status":"ok","timestamp":1653488066453,"user_tz":-540,"elapsed":5,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}},"outputId":"8dca2bb9-5155-4d3f-ec41-49f30d178d5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            multiple                  1280      \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            multiple                  147584    \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            multiple                  147584    \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            multiple                  147584    \n","_________________________________________________________________\n","flatten_1 (Flatten)          multiple                  0         \n","_________________________________________________________________\n","dense_1 (Dense)              multiple                  5308928   \n","_________________________________________________________________\n","dense_2 (Dense)              multiple                  262656    \n","_________________________________________________________________\n","dense_3 (Dense)              multiple                  41553     \n","=================================================================\n","Total params: 6,057,169\n","Trainable params: 6,057,169\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["input()"],"metadata":{"id":"c0B_jiUJkXk7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for episode in tqdm(range(1, episodes+1), unit='episode'):\n","    agent.tensorboard.step = episode\n","\n","    env.reset()\n","    episode_reward = 0\n","    past_n_wins = env.n_wins\n","\n","    done = False\n","    while not done:\n","        current_state = env.state_im\n","\n","        action = agent.get_action(current_state)\n","\n","        new_state, reward, done = env.step(action)\n","\n","        episode_reward += reward\n","\n","        agent.update_replay_memory((current_state, action, reward, new_state, done))\n","        agent.train(done)\n","\n","        n_clicks += 1\n","\n","    progress_list.append(env.n_progress) # n of non-guess moves\n","    ep_rewards.append(episode_reward)\n","\n","    if env.n_wins > past_n_wins:\n","        wins_list.append(1)\n","    else:\n","        wins_list.append(0)\n","\n","    if len(agent.replay_memory) < MEM_SIZE_MIN:\n","        continue\n","\n","    if not episode % AGG_STATS_EVERY:\n","        med_progress = round(np.median(progress_list[-AGG_STATS_EVERY:]), 2)\n","        # 어차피 100번마다 구하니 소수점 아래 3자리부터는 필요가 없다.\n","        win_rate = round(np.sum(wins_list[-AGG_STATS_EVERY:]) / AGG_STATS_EVERY, 2)\n","        med_reward = round(np.median(ep_rewards[-AGG_STATS_EVERY:]), 2)\n","\n","        agent.tensorboard.update_stats(\n","            progress_med = med_progress,\n","            winrate = win_rate,\n","            reward_med = med_reward,\n","            learn_rate = agent.learn_rate,\n","            epsilon = agent.epsilon)\n","\n","        print(f'Episode: {episode}, Median progress: {med_progress}, Median reward: {med_reward}, Win rate : {win_rate}')\n","\n","    if not episode % SAVE_MODEL_EVERY:\n","        with open(f'/content/drive/My Drive/RL_Project/replay/dqn.pkl', 'wb') as output:\n","            pickle.dump(agent.replay_memory, output)\n","\n","        agent.model.save(f'/content/drive/My Drive/RL_Project/models/dqn.h5')\n","        # 10000 episode마다 파일 저장\n","        f = open(\"/content/drive/My Drive/RL_Project/win.txt\", 'w')\n","        f.write(str(env.n_wins))\n","        f.write(str(wins_list))\n","        f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":964},"id":"s6q6uq8lbdh7","outputId":"348bd42e-4e56-42ed-8c68-c007f02da70e","executionInfo":{"status":"error","timestamp":1653488073033,"user_tz":-540,"elapsed":6583,"user":{"displayName":"딤디푼","userId":"13038085312031720056"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 2/100000 [00:06<84:46:19,  3.05s/episode]\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-11-22e6bd5c9ef8>\", line 12, in <module>\n","    action = agent.get_action(current_state)\n","  File \"<ipython-input-8-c88f21a34635>\", line 31, in get_action\n","    moves = self.model.predict(np.reshape(state, (1, self.env.nrows, self.env.ncols, 1)))\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training.py\", line 908, in predict\n","    use_multiprocessing=use_multiprocessing)\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training_arrays.py\", line 723, in predict\n","    callbacks=callbacks)\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training_arrays.py\", line 394, in model_iteration\n","    batch_outs = f(ins_batch)\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\n","    run_metadata=self.run_metadata)\n","  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1472, in __call__\n","    run_metadata_ptr)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 733, in getmodule\n","    if ismodule(module) and hasattr(module, '__file__'):\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"markdown","source":["바뀐 환경은 나중에 풉시다"],"metadata":{"id":"CMDrCbjZY2Mp"}},{"cell_type":"code","source":["# Beginner: (9x9, 10) / Intermediate: (16x16, 40) / Expert: (16x30, 99)\n","width, height, n_mines = 9, 9, 10\n","\n","# rewards = {'win': 1, 'lose': -1, 'progress': 0.3, 'guess': -0.3, 'no_progress': -0.3}\n","env = MinesweeperEnv(width, height, n_mines)\n","agent = DQNAgent(env, conv_units=CONV_UNITS, dense_units=DENSE_UNITS)\n","\n","progress_list, wins_list, ep_rewards = [], [], []\n","n_clicks = 0\n","episodes = 100_000"],"metadata":{"id":"axrI0n7g7E8C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for episode in tqdm(range(1, episodes+1), unit='episode'):\n","    agent.tensorboard.step = episode\n","\n","    env.reset()\n","    episode_reward = 0\n","    past_n_wins = env.n_wins\n","\n","    done = False\n","    while not done:\n","        current_state = env.state_im\n","\n","        print(current_state)\n","        print(agent.get_action(current_state))\n","\n","        action = agent.get_action(current_state)\n","\n","        new_state, reward, done = env.step(action)\n","\n","        episode_reward += reward\n","\n","        agent.update_replay_memory((current_state, action, reward, new_state, done))\n","        agent.train(done)\n","\n","        n_clicks += 1\n","\n","    progress_list.append(env.n_progress) # n of non-guess moves\n","    ep_rewards.append(episode_reward)\n","\n","    if env.n_wins > past_n_wins:\n","        wins_list.append(1)\n","    else:\n","        wins_list.append(0)\n","\n","    if len(agent.replay_memory) < MEM_SIZE_MIN:\n","        continue\n","\n","    if not episode % AGG_STATS_EVERY:\n","        med_progress = round(np.median(progress_list[-AGG_STATS_EVERY:]), 2)\n","        # 어차피 100번마다 구하니 소수점 아래 3자리부터는 필요가 없다.\n","        win_rate = round(np.sum(wins_list[-AGG_STATS_EVERY:]) / AGG_STATS_EVERY, 2)\n","        med_reward = round(np.median(ep_rewards[-AGG_STATS_EVERY:]), 2)\n","\n","        agent.tensorboard.update_stats(\n","            progress_med = med_progress,\n","            winrate = win_rate,\n","            reward_med = med_reward,\n","            learn_rate = agent.learn_rate,\n","            epsilon = agent.epsilon)\n","\n","        print(f'Episode: {episode}, Median progress: {med_progress}, Median reward: {med_reward}, Win rate : {win_rate}')\n","\n","    if not episode % SAVE_MODEL_EVERY:\n","        with open(f'/content/drive/My Drive/RL_Project/replay/dqn_custom.pkl', 'wb') as output:\n","            pickle.dump(agent.replay_memory, output)\n","\n","        agent.model.save(f'/content/drive/My Drive/RL_Project/models/dqn_custom.h5')\n","        # 10000 episode마다 파일 저장\n","        f = open(\"/content/drive/My Drive/RL_Project/custom_win.txt\", 'w')\n","        f.write(str(env.n_wins))\n","        f.write(str(wins_list))\n","        f.close()"],"metadata":{"id":"2RvRcyNzGe-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent.model.save(f'/content/drive/My Drive/RL_Project/models/dqn_custom.h5')"],"metadata":{"id":"1xo89T_PBlbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = open(\"/content/drive/My Drive/RL_Project/win_final_custom.txt\", 'w')\n","f.write(str(env.n_wins))\n","f.write(str(wins_list))\n","f.close()"],"metadata":{"id":"79waGLerltJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","%tensorboard --logdir logs/DQN/"],"metadata":{"id":"uBBAzERoSExk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이거 1분 이상 걸림"],"metadata":{"id":"4WUYf7oM1vHa"}},{"cell_type":"code","source":["with open(\"/content/drive/My Drive/RL_Project/replay/dqn_custom.pkl\", \"rb\") as fr:\n","    data = pickle.load(fr)\n","#print(data)"],"metadata":{"id":"YChRtPa5l17r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent.model.summary()"],"metadata":{"id":"Sc2v64LK1c6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DhxPtkHmnLo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DQNAgent_custom(object):\n","    def __init__(self, env, conv_units, dense_units):\n","        self.env = env\n","\n","        # Deep Q-learning Parameters\n","        self.discount = DISCOUNT\n","        self.learn_rate = learn_rate\n","        self.epsilon = epsilon\n","        self.model = dqn(\n","            self.learn_rate, self.env.state_im.shape, self.env.ntiles, conv_units, dense_units)\n","\n","        # target model - this is what we predict against every step\n","        self.target_model = dqn(\n","            self.learn_rate, self.env.state_im.shape, self.env.ntiles, conv_units, dense_units)\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","        self.replay_memory = deque(maxlen=MEM_SIZE)\n","        self.target_update_counter = 0\n","\n","        self.tensorboard = ModifiedTensorBoard(log_dir=f'logs/DQN')\n","\n","    def get_action(self, state):\n","        board = state.reshape(1, self.env.ntiles)\n","        unsolved = [i for i, x in enumerate(board[0]) if x==-0.125]\n","\n","        rand = np.random.random() # random value b/w 0 & 1\n","\n","        if rand < self.epsilon: # random move (explore)\n","            # move = np.random.choice(unsolved)\n","            if unsolved:\n","                move = np.random.choice(unsolved)\n","            else: # 이때 판에서 랜덤으로 가야할 듯\n","                move = 0\n","        else:\n","            moves = self.model.predict(np.reshape(state, (1, self.env.nrows, self.env.ncols, 1)))\n","            moves[board!=-0.125] = np.min(moves) # set already clicked tiles to min value\n","            move = np.argmax(moves)\n","\n","        return move\n","\n","    def update_replay_memory(self, transition):\n","        self.replay_memory.append(transition)\n","\n","    def train(self, done):\n","        if len(self.replay_memory) < MEM_SIZE_MIN:\n","            return\n","\n","        batch = random.sample(self.replay_memory, BATCH_SIZE)\n","\n","        current_states = np.array([transition[0] for transition in batch])\n","        current_qs_list = self.model.predict(current_states)\n","\n","        new_current_states = np.array([transition[3] for transition in batch])\n","        future_qs_list = self.target_model.predict(new_current_states)\n","\n","        X,y = [], []\n","\n","        for i, (current_state, action, reward, new_current_state, done) in enumerate(batch):\n","            if not done:\n","                max_future_q = np.max(future_qs_list[i])\n","                new_q = reward + DISCOUNT * max_future_q\n","            else:\n","                new_q = reward\n","\n","            current_qs = current_qs_list[i]\n","            current_qs[action] = new_q\n","\n","            X.append(current_state)\n","            y.append(current_qs)\n","\n","        self.model.fit(np.array(X),\n","                       np.array(y),\n","                       batch_size=BATCH_SIZE,\n","                       shuffle=False,\n","                       verbose=0,\n","                       callbacks=[self.tensorboard] if done else None)\n","\n","        # updating to determine if we want to update target_model yet\n","        if done:\n","            self.target_update_counter += 1\n","\n","        if self.target_update_counter > UPDATE_TARGET_EVERY:\n","            self.target_model.set_weights(self.model.get_weights())\n","            self.target_update_counter = 0\n","\n","        # decay learn_rate\n","        self.learn_rate = max(LEARN_MIN, self.learn_rate*LEARN_DECAY)\n","\n","        # decay epsilon\n","        self.epsilon = max(EPSILON_MIN, self.epsilon*EPSILON_DECAY)"],"metadata":{"id":"Jevnnku-COK2"},"execution_count":null,"outputs":[]}]}