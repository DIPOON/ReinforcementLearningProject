{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Minesweeper solver with TRPO\n",
    "\n",
    "References\n",
    "> 1. [(medium) article for stable baselines](https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82)\n",
    "> 1. [(colab) example of medium article](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb)\n",
    "> 1. [(github) minesweeper gym environment](https://github.com/aylint/gym-minesweeper)\n",
    "\n",
    "Helps\n",
    "> 1. [(github) stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "> 1. [(github) stable-baselines3-contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n",
    "> 1. [(github) stable-baselines](https://github.com/hill-a/stable-baselines)\n",
    "> 1. [(doc) stable-baselines](https://stable-baselines.readthedocs.io/en/master/)\n",
    "> 1. [(doc) stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html)\n",
    "> 1. [(doc) stable-baselines3-contrib](https://sb3-contrib.readthedocs.io/en/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32163,
     "status": "ok",
     "timestamp": 1653442503910,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "bC6ssh-iDeWj",
    "outputId": "7dad0010-0612-4180-e6b1-2bd8e440b104"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from stable_baselines3 import DQN, PPO\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#from stable_baselines3.common.callbacks import BaseCallback\n",
    "#from stable_baselines3.common.logger import TensorBoardOutputFormat, configure\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "#from stable_baselines3.her.her_replay_buffer import HerReplayBuffer\n",
    "from typing import Callable\n",
    "\n",
    "from minesweeper_gym_env import MinesweeperEnv\n",
    "from minesweeper_gym_modified import MinesweeperModifiedEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.sample()[None].shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_dim, features_dim, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    #lr0 = initial_value\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining: = 1.0 - (num_timesteps / total_timesteps)\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        if progress_remaining > 0.8:\n",
    "            return initial_value\n",
    "        else:\n",
    "            return progress_remaining * initial_value * 1.25\n",
    "        #return progress_remaining * initial_value\n",
    "        #nonlocal lr0\n",
    "        #lr0 = max(0.001, lr0 * 0.99975) # 0.99975\n",
    "        #return lr0\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward for the last 100 episodes\n",
    "    \"\"\"\n",
    "    episode_rewards = [0.0]\n",
    "    episode_wins = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "        #if i % 100 == 1:\n",
    "        #    print('Playing episode {}'.format(i))\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_rewards[-1] += reward\n",
    "            if done:\n",
    "                episode_wins.append(info.get('is_success'))\n",
    "                break\n",
    "            elif info.get('num_actions') > 200:\n",
    "                #print('Episode {}. Over action in obs, action: \\n{}, {}'.format(i, obs, action))\n",
    "                episode_wins.append(False)\n",
    "                break\n",
    "    \"\"\"\n",
    "    # Compute mean reward for the last 100 episodes\n",
    "    mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
    "    print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
    "    \"\"\"\n",
    "    win_rate = round(np.mean(episode_wins), 2)\n",
    "    print(\"Win rates:\", win_rate, \"Num episodes:\", len(episode_wins))\n",
    "    \n",
    "    return episode_rewards, episode_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO for Original Minesweeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1652343522531,
     "user": {
      "displayName": "이경헌",
      "userId": "12732568205041741847"
     },
     "user_tz": -540
    },
    "id": "wP7-tRuxWG07",
    "outputId": "e382649f-b819-442d-cc6b-e8d1f8bf73e7"
   },
   "outputs": [],
   "source": [
    "env = MinesweeperEnv(9, 10)\n",
    "model = TRPO('CnnPolicy', env, \n",
    "            learning_rate=linear_schedule(0.001),  \n",
    "            batch_size=64, \n",
    "            gamma=0.1, \n",
    "            #train_freq=(1, 'episode'), \n",
    "            #buffer_size=4,\n",
    "            tensorboard_log=\"./trpo_tensorboard/\",\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=0\n",
    "           )\n",
    "model.learn(total_timesteps=int(5e6), \n",
    "            log_interval=10,\n",
    "            tb_log_name='s9m10',\n",
    "            #eval_log_path='eval_test',\n",
    "            reset_num_timesteps=True)\n",
    "model.save(\"trpo_minesweeper_s9m10\")\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fzRz8Iv-XSAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.9 Num episodes: 1000\n",
      "mean_reward: 1.08\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"trpo_minesweeper_s4m1\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.37 Num episodes: 1000\n",
      "mean_reward: 1.5\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"trpo_minesweeper_s5m3\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.0 Num episodes: 1000\n",
      "mean_reward: 1.5\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"trpo_minesweeper_s9m10\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO for Modified Minesweeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MinesweeperModifiedEnv(5, 3)\n",
    "model = TRPO('CnnPolicy', env, \n",
    "            learning_rate=linear_schedule(0.001),  \n",
    "            batch_size=64, \n",
    "            gamma=0.1, \n",
    "            #train_freq=(1, 'episode'), \n",
    "            #buffer_size=4,\n",
    "            tensorboard_log=\"./trpo_tensorboard/\",\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=0\n",
    "           )\n",
    "model.learn(total_timesteps=int(5e5), \n",
    "            log_interval=10,\n",
    "            tb_log_name='modified_s5m3',\n",
    "            #eval_log_path='eval_test',\n",
    "            reset_num_timesteps=True)\n",
    "model.save(\"modified_trpo_minesweeper_s5m3\")\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.72 Num episodes: 1000\n",
      "mean_reward: 6.75\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"modified_trpo_minesweeper_s4m1\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rates: 0.04 Num episodes: 1000\n",
      "mean_reward: 7.99\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"modified_trpo_minesweeper_s5m3\")\n",
    "episode_rewards, episode_wins = evaluate(model, env=env, num_episodes=1000)\n",
    "mean_reward = round(np.mean(episode_rewards), 2)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653436025115,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "MthXfoFIkDL-"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1653436083595,
     "user": {
      "displayName": "Gyeongheon Lee",
      "userId": "09300670283340891360"
     },
     "user_tz": -540
    },
    "id": "hnv4G8QQkAcX",
    "outputId": "02545a85-eba9-4235-c565-277622e7317b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1\n",
      "0\t1\t1\t1\t\n",
      "0\t1\t-1\t1\t\n",
      "1\t1\t1\t1\t\n",
      "-2\t-2\t1\t0\tFalse\n",
      "[[[False False False False]\n",
      "  [False False  True False]\n",
      "  [False False False False]\n",
      "  [ True  True False False]]]\n",
      "\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "if done:\n",
    "    obs = env.reset()\n",
    "    print('New Game!')\n",
    "row, col = 3, 2\n",
    "\n",
    "action = 4 * row + col\n",
    "obs, reward, done, info = env.step(action)\n",
    "print('reward: {}'.format(reward))\n",
    "env.render()\n",
    "print(info.get('is_success'))\n",
    "print(env.board == -1)\n",
    "if done:\n",
    "    print('\\nGame Over!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d2atE5vL8L2g",
    "BEazdwPFKZun",
    "9x47HdO6KPQ8",
    "Nsju026AQg0z",
    "AeDD26lyVwJP"
   ],
   "name": "sketchbook.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb",
     "timestamp": 1652336425341
    }
   ]
  },
  "kernelspec": {
   "display_name": "sketchbook",
   "language": "python",
   "name": "sketchbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
